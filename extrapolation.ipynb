{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [1000, 1], strides() = [1, 0]\n",
      "param.sizes() = [1000, 1], strides() = [1, 0] (Triggered internally at ..\\torch/csrc/autograd/functions/accumulate_grad.h:219.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS Iteration 10, Loss: 0.42084017395973206\n",
      "LBFGS Iteration 20, Loss: 0.010974712669849396\n",
      "LBFGS Iteration 30, Loss: 0.00333099951967597\n",
      "LBFGS Iteration 40, Loss: 0.001702727866359055\n",
      "LBFGS Iteration 50, Loss: 0.0011226300848647952\n",
      "LBFGS Iteration 60, Loss: 0.0009786197915673256\n",
      "LBFGS Iteration 70, Loss: 0.0008482502307742834\n",
      "LBFGS Iteration 80, Loss: 0.0007915855967439711\n",
      "LBFGS Iteration 90, Loss: 0.0006918628350831568\n",
      "LBFGS Iteration 100, Loss: 0.0006351459305733442\n",
      "LBFGS Iteration 110, Loss: 0.0006008865311741829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 190\u001b[0m\n\u001b[0;32m    186\u001b[0m         g_residual_history\u001b[38;5;241m.\u001b[39mappend(g_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m--> 190\u001b[0m     optimizer_lbfgs\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Plot the loss curve\u001b[39;00m\n\u001b[0;32m    193\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\optim\\lbfgs.py:433\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[1;32m--> 433\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m _strong_wolfe(\n\u001b[0;32m    434\u001b[0m         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m    436\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\optim\\lbfgs.py:49\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[1;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[0;32m     47\u001b[0m g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m obj_func(x, t, d)\n\u001b[0;32m     50\u001b[0m ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     51\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\optim\\lbfgs.py:431\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[1;34m(x, t, d)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\optim\\lbfgs.py:285\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[1;34m(self, closure, x, t, d)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m--> 285\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(closure())\n\u001b[0;32m    286\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[2], line 158\u001b[0m, in \u001b[0;36mclosure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m lbfgs_iter, best_loss, patience_counter  \u001b[38;5;66;03m# Access the global iteration variables\u001b[39;00m\n\u001b[0;32m    156\u001b[0m optimizer_lbfgs\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 158\u001b[0m u_prediction, v_prediction, p_prediction, f_prediction, g_prediction \u001b[38;5;241m=\u001b[39m create_pde(x_batch, y_batch, t_batch)\n\u001b[0;32m    159\u001b[0m u_loss \u001b[38;5;241m=\u001b[39m mse(u_prediction, u_batch)\n\u001b[0;32m    160\u001b[0m v_loss \u001b[38;5;241m=\u001b[39m mse(v_prediction, v_batch)\n",
      "Cell \u001b[1;32mIn[2], line 108\u001b[0m, in \u001b[0;36mcreate_pde\u001b[1;34m(x, y, t)\u001b[0m\n\u001b[0;32m    105\u001b[0m res \u001b[38;5;241m=\u001b[39m net(torch\u001b[38;5;241m.\u001b[39mhstack((x, y, t)))\n\u001b[0;32m    106\u001b[0m psi, p \u001b[38;5;241m=\u001b[39m res[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], res[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m--> 108\u001b[0m u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(psi, y, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(psi), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]  \n\u001b[0;32m    109\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(psi, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(psi), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    111\u001b[0m u_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    409\u001b[0m         grad_outputs_\n\u001b[0;32m    410\u001b[0m     )\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[0;32m    413\u001b[0m         t_outputs,\n\u001b[0;32m    414\u001b[0m         grad_outputs_,\n\u001b[0;32m    415\u001b[0m         retain_graph,\n\u001b[0;32m    416\u001b[0m         create_graph,\n\u001b[0;32m    417\u001b[0m         inputs,\n\u001b[0;32m    418\u001b[0m         allow_unused,\n\u001b[0;32m    419\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    420\u001b[0m     )\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    425\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' This code follows the PINN - Navier Stokes project done by Computational Domain, but adapted for our secific scenarios'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "nu = 0.01\n",
    "batch_size = 1000  # Define a batch size to control memory usage\n",
    "\n",
    "# Load data and create training dataset\n",
    "data = scipy.io.loadmat('cylinder_wake.mat')\n",
    "\n",
    "U_star = data['U_star']  # N x 2 x T\n",
    "P_star = data['p_star']  # N x T\n",
    "t_star = data['t']  # T x 1\n",
    "X_star = data['X_star']  # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# Split data into training (first 70%) and testing (last 30%) sets based on timesteps\n",
    "train_T = int(0.7 * T)\n",
    "test_T = T - train_T\n",
    "\n",
    "# Rearrange Data for Training\n",
    "XX_train = np.tile(X_star[:, 0:1], (1, train_T))  # N x train_T\n",
    "YY_train = np.tile(X_star[:, 1:2], (1, train_T))  # N x train_T\n",
    "TT_train = np.tile(t_star[:train_T], (1, N)).T  # N x train_T\n",
    "\n",
    "UU_train = U_star[:, 0, :train_T]  # N x train_T\n",
    "VV_train = U_star[:, 1, :train_T]  # N x train_T\n",
    "PP_train = P_star[:, :train_T]  # N x train_T\n",
    "\n",
    "x_train = torch.tensor(XX_train.flatten()[:, None], dtype=torch.float32, requires_grad=True)\n",
    "y_train = torch.tensor(YY_train.flatten()[:, None], dtype=torch.float32, requires_grad=True)\n",
    "t_train = torch.tensor(TT_train.flatten()[:, None], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "u_train = torch.tensor(UU_train.flatten()[:, None], dtype=torch.float32)\n",
    "v_train = torch.tensor(VV_train.flatten()[:, None], dtype=torch.float32)\n",
    "p_train = torch.tensor(PP_train.flatten()[:, None], dtype=torch.float32)\n",
    "\n",
    "# Rearrange Data for Testing\n",
    "XX_test = np.tile(X_star[:, 0:1], (1, test_T))  # N x test_T\n",
    "YY_test = np.tile(X_star[:, 1:2], (1, test_T))  # N x test_T\n",
    "TT_test = np.tile(t_star[train_T:], (1, N)).T  # N x test_T\n",
    "\n",
    "UU_test = U_star[:, 0, train_T:]  # N x test_T\n",
    "VV_test = U_star[:, 1, train_T:]  # N x test_T\n",
    "PP_test = P_star[:, train_T:]  # N x test_T\n",
    "\n",
    "x_test = torch.tensor(XX_test.flatten()[:, None], dtype=torch.float32, requires_grad=True)\n",
    "y_test = torch.tensor(YY_test.flatten()[:, None], dtype=torch.float32, requires_grad=True)\n",
    "t_test = torch.tensor(TT_test.flatten()[:, None], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "u_test = torch.tensor(UU_test.flatten()[:, None], dtype=torch.float32)\n",
    "v_test = torch.tensor(VV_test.flatten()[:, None], dtype=torch.float32)\n",
    "p_test = torch.tensor(PP_test.flatten()[:, None], dtype=torch.float32)\n",
    "\n",
    "# Define a simple ResNet block\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x)) + x\n",
    "\n",
    "# Define the ResNet-style neural network with 7 hidden layers\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input_layer = nn.Linear(3, 20)\n",
    "        self.res_block1 = ResNetBlock(20, 20)\n",
    "        self.res_block2 = ResNetBlock(20, 20)\n",
    "        self.res_block3 = ResNetBlock(20, 20)\n",
    "        self.res_block4 = ResNetBlock(20, 20)\n",
    "        self.res_block5 = ResNetBlock(20, 20)\n",
    "        self.res_block6 = ResNetBlock(20, 20)\n",
    "        self.res_block7 = ResNetBlock(20, 20)\n",
    "        self.output_layer = nn.Linear(20, 3)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        x = self.res_block5(x)\n",
    "        x = self.res_block6(x)\n",
    "        x = self.res_block7(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "net = ResNet()\n",
    "\n",
    "# Define the loss function\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Function written by Computational Domain\n",
    "def create_pde(x, y, t):\n",
    "    res = net(torch.hstack((x, y, t)))\n",
    "    psi, p = res[:, 0:1], res[:, 1:2]\n",
    "\n",
    "    u = torch.autograd.grad(psi, y, grad_outputs=torch.ones_like(psi), create_graph=True)[0]  \n",
    "    v = -1. * torch.autograd.grad(psi, x, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
    "\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "\n",
    "    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "\n",
    "    f = u_t + u * u_x + v * u_y + p_x - nu * (u_xx + u_yy)\n",
    "    g = v_t + u * v_x + v * v_y + p_y - nu * (v_xx + v_yy)\n",
    "\n",
    "    return u, v, p, f, g\n",
    "\n",
    "# Use LBFGS optimizer with early stopping\n",
    "optimizer_lbfgs = torch.optim.LBFGS(net.parameters(), lr=1, max_iter=20000, max_eval=50000,\n",
    "                                    history_size=50, tolerance_grad=1e-05, tolerance_change=0.5 * np.finfo(float).eps,\n",
    "                                    line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "loss_history = []\n",
    "f_residual_history = []\n",
    "g_residual_history = []\n",
    "lbfgs_iter = 0  # Initialize the LBFGS iteration counter\n",
    "patience = 200  # Number of iterations with no significant improvement before stopping\n",
    "min_delta = 1e-4  # Minimum change to qualify as an improvement\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Fine-tuning with LBFGS and early stopping in batches\n",
    "for i in range(0, x_train.shape[0], batch_size):\n",
    "    x_batch = x_train[i:i+batch_size].detach().clone().requires_grad_(True)\n",
    "    y_batch = y_train[i:i+batch_size].detach().clone().requires_grad_(True)\n",
    "    t_batch = t_train[i:i+batch_size].detach().clone().requires_grad_(True)\n",
    "    u_batch = u_train[i:i+batch_size]\n",
    "    v_batch = v_train[i:i+batch_size]\n",
    "    p_batch = p_train[i:i+batch_size]\n",
    "    \n",
    "    def closure():\n",
    "        global lbfgs_iter, best_loss, patience_counter  # Access the global iteration variables\n",
    "        optimizer_lbfgs.zero_grad()\n",
    "\n",
    "        u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = create_pde(x_batch, y_batch, t_batch)\n",
    "        u_loss = mse(u_prediction, u_batch)\n",
    "        v_loss = mse(v_prediction, v_batch)\n",
    "        p_loss = mse(p_prediction, p_batch)\n",
    "        f_loss = mse(f_prediction, torch.zeros_like(f_prediction))\n",
    "        g_loss = mse(g_prediction, torch.zeros_like(g_prediction))\n",
    "\n",
    "        loss = u_loss + v_loss + p_loss + f_loss + g_loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Print the current iteration and loss\n",
    "        lbfgs_iter += 1\n",
    "        if lbfgs_iter % 10 == 0:\n",
    "            print(f'LBFGS Iteration {lbfgs_iter}, Loss: {loss.item()}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if loss.item() < best_loss - min_delta:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                return loss\n",
    "\n",
    "        # Track loss and residuals\n",
    "        loss_history.append(loss.item())\n",
    "        f_residual_history.append(f_loss.item())\n",
    "        g_residual_history.append(g_loss.item())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    optimizer_lbfgs.step(closure)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "# Plot f and g residuals\n",
    "plt.figure()\n",
    "plt.plot(f_residual_history, label='f Residual')\n",
    "plt.plot(g_residual_history, label='g Residual')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('f and g Residuals Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), 'pinn_70_30_resnet_lbfgs.pt')\n",
    "\n",
    "# Load the trained model and evaluate\n",
    "net.eval()\n",
    "\n",
    "u_out, v_out, p_out, f_out, g_out = create_pde(x_test, y_test, t_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
