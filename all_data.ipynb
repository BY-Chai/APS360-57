{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m u, v, p, f, g\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Train the model using Adam and LBFGS\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m optimizer_adam \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m    121\u001b[0m optimizer_lbfgs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mLBFGS(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m, max_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m,\n\u001b[0;32m    122\u001b[0m                                     history_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, tolerance_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-05\u001b[39m, tolerance_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    123\u001b[0m                                     line_search_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrong_wolfe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, defaults)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:284\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    281\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_param_group(cast(\u001b[38;5;28mdict\u001b[39m, param_group))\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disable_current_modes\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traceback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_traceback_short\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, exc, trace_rules\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompilerFn\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_dead_code, remove_pointless_jumps\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py:50\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _config_module\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getfile, hashable, NP_SUPPORTED_MODULES, unwrap_if_wrapper\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     51\u001b[0m     BuiltinVariable,\n\u001b[0;32m     52\u001b[0m     FunctorchHigherOrderVariable,\n\u001b[0;32m     53\u001b[0m     NestedUserFunctionVariable,\n\u001b[0;32m     54\u001b[0m     SkipFunctionVariable,\n\u001b[0;32m     55\u001b[0m     TorchInGraphFunctionVariable,\n\u001b[0;32m     56\u001b[0m     UserFunctionVariable,\n\u001b[0;32m     57\u001b[0m     UserMethodVariable,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VariableTracker\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03mMap of function objects to their tracing rules (Dynamo variables).\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m* TorchInGraphFunctionVariable: The functions should be put into the FX graph or can be constant folded. E.g.,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\variables\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: ignore-errors\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VariableTracker\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuiltin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BuiltinVariable\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConstantVariable, EnumVariable\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mctx_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     ContextWrappingVariable,\n\u001b[0;32m      8\u001b[0m     DeterministicAlgorithmsVariable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     WithExitFunctionVariable,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\variables\\builtin.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MutableLocal, typestr, VariableTracker\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConstantVariable\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mctx_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventVariable, StreamVariable\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdicts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     44\u001b[0m     ConstDictVariable,\n\u001b[0;32m     45\u001b[0m     DefaultDictVariable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     SetVariable,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlists\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     51\u001b[0m     BaseListVariable,\n\u001b[0;32m     52\u001b[0m     ListIteratorVariable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     TupleVariable,\n\u001b[0;32m     57\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\variables\\ctx_manager.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variables\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_call_function, create_instruction\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_interface_for_device\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unimplemented, Unsupported\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GuardBuilder, install_guard\n",
      "File \u001b[1;32mc:\\Users\\zhyy2\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\device_interface.py:198\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m device_interfaces\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    197\u001b[0m register_interface_for_device(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, CudaInterface)\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()):\n\u001b[0;32m    199\u001b[0m     register_interface_for_device(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, CudaInterface)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' This code follows the PINN - Navier Stokes project done by Computational Domain, but adapted for our secific scenarios'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "nu = 0.01\n",
    "\n",
    "# Load data and create training dataset\n",
    "data = scipy.io.loadmat('cylinder_wake.mat')\n",
    "\n",
    "U_star = data['U_star']  # N x 2 x T\n",
    "P_star = data['p_star']  # N x T\n",
    "t_star = data['t']  # T x 1\n",
    "X_star = data['X_star']  # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# Rearrange Data\n",
    "XX = np.tile(X_star[:, 0:1], (1, T))  # N x T\n",
    "YY = np.tile(X_star[:, 1:2], (1, T))  # N x T\n",
    "TT = np.tile(t_star, (1, N)).T  # N x T\n",
    "\n",
    "UU = U_star[:, 0, :]  # N x T\n",
    "VV = U_star[:, 1, :]  # N x T\n",
    "PP = P_star  # N x T\n",
    "\n",
    "x = XX.flatten()[:, None]  # NT x 1\n",
    "y = YY.flatten()[:, None]  # NT x 1\n",
    "t = TT.flatten()[:, None]  # NT x 1\n",
    "\n",
    "u = UU.flatten()[:, None]  # NT x 1\n",
    "v = VV.flatten()[:, None]  # NT x 1\n",
    "p = PP.flatten()[:, None]  # NT x 1\n",
    "\n",
    "# Randomly sample data without using N_train\n",
    "idx = np.random.choice(N * T, 5000, replace=False)\n",
    "x_train = torch.tensor(x[idx, :], dtype=torch.float32, requires_grad=True)\n",
    "y_train = torch.tensor(y[idx, :], dtype=torch.float32, requires_grad=True)\n",
    "t_train = torch.tensor(t[idx, :], dtype=torch.float32, requires_grad=True)\n",
    "u_train = torch.tensor(u[idx, :], dtype=torch.float32)\n",
    "v_train = torch.tensor(v[idx, :], dtype=torch.float32)\n",
    "p_train = torch.tensor(p[idx, :], dtype=torch.float32)\n",
    "\n",
    "# Define a simple ResNet block\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x)) + x\n",
    "\n",
    "# Define the ResNet-style neural network with 7 hidden layers\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input_layer = nn.Linear(3, 20)\n",
    "        self.res_block1 = ResNetBlock(20, 20)\n",
    "        self.res_block2 = ResNetBlock(20, 20)\n",
    "        self.res_block3 = ResNetBlock(20, 20)\n",
    "        self.res_block4 = ResNetBlock(20, 20)\n",
    "        self.res_block5 = ResNetBlock(20, 20)\n",
    "        self.res_block6 = ResNetBlock(20, 20)\n",
    "        self.res_block7 = ResNetBlock(20, 20)\n",
    "        self.output_layer = nn.Linear(20, 3)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        x = self.res_block5(x)\n",
    "        x = self.res_block6(x)\n",
    "        x = self.res_block7(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "net = ResNet()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "# This function was created by Computational Domain\n",
    "def create_pde(x, y, t):\n",
    "    res = net(torch.hstack((x, y, t)))\n",
    "    psi, p = res[:, 0:1], res[:, 1:2]\n",
    "\n",
    "    u = torch.autograd.grad(psi, y, grad_outputs=torch.ones_like(psi), create_graph=True)[0]  \n",
    "    v = -1. * torch.autograd.grad(psi, x, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
    "\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "\n",
    "    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "\n",
    "    f = u_t + u * u_x + v * u_y + p_x - nu * (u_xx + u_yy)\n",
    "    g = v_t + u * v_x + v * v_y + p_y - nu * (v_xx + v_yy)\n",
    "\n",
    "    return u, v, p, f, g\n",
    "\n",
    "# Train the model using Adam and LBFGS\n",
    "optimizer_adam = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer_lbfgs = torch.optim.LBFGS(net.parameters(), lr=1, max_iter=20000, max_eval=50000,\n",
    "                                    history_size=50, tolerance_grad=1e-05, tolerance_change=0.5 * np.finfo(float).eps,\n",
    "                                    line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "loss_history = []\n",
    "lbfgs_iter = 0  # Initialize the LBFGS iteration counter\n",
    "patience = 200  # Number of iterations with no significant improvement before stopping\n",
    "min_delta = 1e-4  # Minimum change to qualify as an improvement\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop with Adam\n",
    "# for epoch in range(10000):\n",
    "#     def closure():\n",
    "#         optimizer_adam.zero_grad()\n",
    "\n",
    "#         u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = create_pde(x_train, y_train, t_train)\n",
    "#         u_loss = mse(u_prediction, u_train)\n",
    "#         v_loss = mse(v_prediction, v_train)\n",
    "#         p_loss = mse(p_prediction, p_train)\n",
    "#         f_loss = mse(f_prediction, torch.zeros_like(f_prediction))\n",
    "#         g_loss = mse(g_prediction, torch.zeros_like(g_prediction))\n",
    "\n",
    "#         loss = u_loss + v_loss + p_loss + f_loss + g_loss\n",
    "#         loss.backward()\n",
    "#         return loss\n",
    "    \n",
    "#     optimizer_adam.step(closure)\n",
    "#     loss = closure().item()\n",
    "#     loss_history.append(loss)\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'Adam Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Fine-tuning with LBFGS and early stopping\n",
    "def closure():\n",
    "    global lbfgs_iter, best_loss, patience_counter  # Access the global iteration variables\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "\n",
    "    u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = create_pde(x_train, y_train, t_train)\n",
    "    u_loss = mse(u_prediction, u_train)\n",
    "    v_loss = mse(v_prediction, v_train)\n",
    "    p_loss = mse(p_prediction, p_train)\n",
    "    f_loss = mse(f_prediction, torch.zeros_like(f_prediction))\n",
    "    g_loss = mse(g_prediction, torch.zeros_like(g_prediction))\n",
    "\n",
    "    loss = u_loss + v_loss + p_loss + f_loss + g_loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Print the current iteration and loss\n",
    "    lbfgs_iter += 1\n",
    "    if lbfgs_iter % 10 == 0:\n",
    "        print(f'LBFGS Iteration {lbfgs_iter}, Loss: {loss.item()}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    if loss.item() < best_loss - min_delta:\n",
    "        best_loss = loss.item()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            return loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "optimizer_lbfgs.step(closure)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), 'pinn_pretrained_resnet.pt')\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "# Load the trained model and evaluate\n",
    "net.eval()\n",
    "\n",
    "x_test = torch.tensor(X_star[:, 0:1], dtype=torch.float32, requires_grad=True)\n",
    "y_test = torch.tensor(X_star[:, 1:2], dtype=torch.float32, requires_grad=True)\n",
    "t_test = torch.tensor(np.ones((X_star.shape[0], 1)), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "u_out, v_out, p_out, f_out, g_out = create_pde(x_test, y_test, t_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "C:\\Users\\zhyy2\\AppData\\Local\\Temp\\ipykernel_7844\\2093164739.py:21: MatplotlibDeprecationWarning: The collections attribute was deprecated in Matplotlib 3.8 and will be removed two minor releases later.\n",
      "  for c in contour.collections:\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "C:\\Users\\zhyy2\\AppData\\Local\\Temp\\ipykernel_7844\\2093164739.py:21: MatplotlibDeprecationWarning: The collections attribute was deprecated in Matplotlib 3.8 and will be removed two minor releases later.\n",
      "  for c in contour.collections:\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "C:\\Users\\zhyy2\\AppData\\Local\\Temp\\ipykernel_7844\\2093164739.py:21: MatplotlibDeprecationWarning: The collections attribute was deprecated in Matplotlib 3.8 and will be removed two minor releases later.\n",
      "  for c in contour.collections:\n"
     ]
    }
   ],
   "source": [
    "# Function to create and save an animation for a specific field\n",
    "def create_animation(field_name, values_fn, vmin, vmax, filename):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Initialize the first frame\n",
    "    t = torch.tensor(np.full(x_test.shape, t_star[0]), dtype=torch.float32, requires_grad=True)\n",
    "    u_out, v_out, p_out, _, _ = create_pde(x_test, y_test, t)\n",
    "    field_values = values_fn(u_out, v_out, p_out).detach().cpu().numpy().flatten()\n",
    "    \n",
    "    contour = ax.tricontourf(x_test.detach().numpy().flatten(), y_test.detach().numpy().flatten(), field_values, levels=20, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    colorbar = fig.colorbar(contour, ax=ax, label=field_name)  # Add colorbar once\n",
    "\n",
    "    ax.set_xlim(1, 8)  # Set x-axis limits\n",
    "    ax.set_ylim(-2, 2)  # Set y-axis limits\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'{field_name} at t = {t_star[0][0]}')\n",
    "\n",
    "    def animate(i):\n",
    "        nonlocal contour\n",
    "        for c in contour.collections:\n",
    "            c.remove()\n",
    "        t = torch.tensor(np.full(x_test.shape, t_star[i]), dtype=torch.float32, requires_grad=True)\n",
    "        u_out, v_out, p_out, _, _ = create_pde(x_test, y_test, t)\n",
    "        field_values = values_fn(u_out, v_out, p_out).detach().cpu().numpy().flatten()\n",
    "        contour = ax.tricontourf(x_test.detach().numpy().flatten(), y_test.detach().numpy().flatten(), field_values, levels=20, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f'{field_name} at t = {t_star[i][0]}')\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=T, interval=50, blit=False)\n",
    "    ani.save(filename, writer='imagemagick', fps=10)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Determine value ranges for proper color scaling\n",
    "t = torch.tensor(np.full(x_test.shape, t_star[0]), dtype=torch.float32, requires_grad=True)\n",
    "u_out, v_out, p_out, _, _ = create_pde(x_test, y_test, t)  # Extract only the first three values\n",
    "\n",
    "vmin_p, vmax_p = torch.min(p_out).item(), torch.max(p_out).item()\n",
    "vmin_u, vmax_u = torch.min(u_out).item(), torch.max(u_out).item()\n",
    "vmin_v, vmax_v = torch.min(v_out).item(), torch.max(v_out).item()\n",
    "\n",
    "# Create and save animations for each field\n",
    "create_animation('Pressure Field', lambda u_out, v_out, p_out: p_out, vmin_p, vmax_p, 'pressure_animation.gif')\n",
    "create_animation('U Velocity Field', lambda u_out, v_out, p_out: u_out, vmin_u, vmax_u, 'u_velocity_animation.gif')\n",
    "create_animation('V Velocity Field', lambda u_out, v_out, p_out: v_out, vmin_v, vmax_v, 'v_velocity_animation.gif')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
