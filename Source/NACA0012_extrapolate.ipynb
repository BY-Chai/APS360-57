{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "nu = 0.01 # From very high Reynolds number\n",
    "\n",
    "# Load data and create training dataset\n",
    "file = 'Cleaned_APS360_0012_dt0001_2.mat'\n",
    "data = scipy.io.loadmat(f\"./Data/{file}\")\n",
    "\n",
    "##### Bounds for training ######\n",
    "start_time = 0\n",
    "end_time = 140\n",
    "\n",
    "U_star = data['U_star'][...,start_time:end_time]  # N x 2 x T\n",
    "P_star = data['p_star'][...,start_time:end_time]  # N x T\n",
    "t_star = data['t'][start_time:end_time]  # T x 1\n",
    "X_star = data['X_star']  # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# Rearrange Data\n",
    "XX = np.tile(X_star[:, 0:1], (1, T))  # N x T\n",
    "YY = np.tile(X_star[:, 1:2], (1, T))  # N x T\n",
    "TT = np.tile(t_star, (1, N)).T  # N x T\n",
    "\n",
    "UU = U_star[:, 0, :]  # N x T\n",
    "VV = U_star[:, 1, :]  # N x T\n",
    "PP = P_star  # N x T\n",
    "\n",
    "x = XX.flatten()[:, None]  # NT x 1\n",
    "y = YY.flatten()[:, None]  # NT x 1\n",
    "t = TT.flatten()[:, None]  # NT x 1\n",
    "\n",
    "u = UU.flatten()[:, None]  # NT x 1\n",
    "v = VV.flatten()[:, None]  # NT x 1\n",
    "p = PP.flatten()[:, None]  # NT x 1\n",
    "\n",
    "# Randomly sample data without using N_train\n",
    "idx = np.random.choice(N * T, 5000, replace=False)\n",
    "x_train = torch.tensor(x[idx, :], dtype=torch.float32, requires_grad=True)\n",
    "y_train = torch.tensor(y[idx, :], dtype=torch.float32, requires_grad=True)\n",
    "t_train = torch.tensor(t[idx, :], dtype=torch.float32, requires_grad=True)\n",
    "u_train = torch.tensor(u[idx, :], dtype=torch.float32)\n",
    "v_train = torch.tensor(v[idx, :], dtype=torch.float32)\n",
    "p_train = torch.tensor(p[idx, :], dtype=torch.float32)\n",
    "\n",
    "# Define a simple ResNet block\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x)) + x\n",
    "\n",
    "# Define the ResNet-style neural network with 7 hidden layers\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input_layer = nn.Linear(3, 100)\n",
    "        self.res_block1 = ResNetBlock(100, 100)\n",
    "        self.res_block2 = ResNetBlock(100, 100)\n",
    "        self.res_block3 = ResNetBlock(100, 100)\n",
    "        self.res_block4 = ResNetBlock(100, 100)\n",
    "        self.res_block5 = ResNetBlock(100, 100)\n",
    "        self.res_block6 = ResNetBlock(100, 100)\n",
    "        self.res_block7 = ResNetBlock(100, 100)\n",
    "        self.res_block8 = ResNetBlock(100, 100)\n",
    "        self.res_block9 = ResNetBlock(100, 100)\n",
    "        self.res_block10 = ResNetBlock(100, 100)\n",
    "        self.res_block11 = ResNetBlock(100, 100)\n",
    "        self.res_block12 = ResNetBlock(100, 100)\n",
    "        self.res_block13 = ResNetBlock(100, 100)\n",
    "        self.res_block14 = ResNetBlock(100, 100)\n",
    "        self.res_block15 = ResNetBlock(100, 100)\n",
    "        self.res_block16 = ResNetBlock(100, 100)\n",
    "        self.res_block17 = ResNetBlock(100, 100)\n",
    "        self.res_block18 = ResNetBlock(100, 100)\n",
    "        self.res_block19 = ResNetBlock(100, 100)\n",
    "        self.res_block20 = ResNetBlock(100, 100)\n",
    "        self.output_layer = nn.Linear(100, 3)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        x = self.res_block5(x)\n",
    "        x = self.res_block6(x)\n",
    "        x = self.res_block7(x)\n",
    "        x = self.res_block8(x)\n",
    "        x = self.res_block9(x)\n",
    "        x = self.res_block10(x)\n",
    "        x = self.res_block11(x)\n",
    "        x = self.res_block12(x)\n",
    "        x = self.res_block13(x)\n",
    "        x = self.res_block14(x)\n",
    "        x = self.res_block15(x)\n",
    "        x = self.res_block16(x)\n",
    "        x = self.res_block17(x)\n",
    "        x = self.res_block18(x)\n",
    "        x = self.res_block19(x)\n",
    "        x = self.res_block20(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "net = ResNet()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def create_pde(x, y, t):\n",
    "    res = net(torch.hstack((x, y, t)))\n",
    "    psi, p = res[:, 0:1], res[:, 1:2]\n",
    "\n",
    "    u = torch.autograd.grad(psi, y, grad_outputs=torch.ones_like(psi), create_graph=True)[0]  \n",
    "    v = -1. * torch.autograd.grad(psi, x, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
    "\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "\n",
    "    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "\n",
    "    f = u_t + u * u_x + v * u_y + p_x - nu * (u_xx + u_yy)\n",
    "    g = v_t + u * v_x + v * v_y + p_y - nu * (v_xx + v_yy)\n",
    "\n",
    "    return u, v, p, f, g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS Iteration 10, Loss: 1997514.875\n",
      "LBFGS Iteration 20, Loss: 1985703.875\n",
      "LBFGS Iteration 30, Loss: 1967086.75\n",
      "LBFGS Iteration 40, Loss: 1951659.25\n",
      "LBFGS Iteration 50, Loss: 1938169.25\n",
      "LBFGS Iteration 60, Loss: 1921280.125\n",
      "LBFGS Iteration 70, Loss: 1911816.75\n",
      "LBFGS Iteration 80, Loss: 1900934.375\n",
      "LBFGS Iteration 90, Loss: 1892731.875\n",
      "LBFGS Iteration 100, Loss: 1882844.375\n",
      "LBFGS Iteration 110, Loss: 1867839.5\n",
      "LBFGS Iteration 120, Loss: 1859273.625\n",
      "LBFGS Iteration 130, Loss: 1842253.625\n",
      "LBFGS Iteration 140, Loss: 1834127.25\n",
      "LBFGS Iteration 150, Loss: 1826839.0\n",
      "LBFGS Iteration 160, Loss: 1823396.875\n",
      "LBFGS Iteration 170, Loss: 1817841.0\n",
      "LBFGS Iteration 180, Loss: 1811191.125\n",
      "LBFGS Iteration 190, Loss: 1800950.625\n",
      "LBFGS Iteration 200, Loss: 1795566.25\n",
      "LBFGS Iteration 210, Loss: 1788658.625\n",
      "LBFGS Iteration 220, Loss: 1786664.625\n",
      "LBFGS Iteration 230, Loss: 1782564.375\n",
      "LBFGS Iteration 240, Loss: 1781005.25\n",
      "LBFGS Iteration 250, Loss: 1779708.25\n",
      "LBFGS Iteration 260, Loss: 1778404.125\n",
      "LBFGS Iteration 270, Loss: 1776491.125\n",
      "LBFGS Iteration 280, Loss: 1774614.75\n",
      "LBFGS Iteration 290, Loss: 1773492.375\n",
      "LBFGS Iteration 300, Loss: 1772153.25\n",
      "LBFGS Iteration 310, Loss: 1771721.0\n",
      "LBFGS Iteration 320, Loss: 1771286.5\n",
      "LBFGS Iteration 330, Loss: 1769459.375\n",
      "LBFGS Iteration 340, Loss: 1768260.125\n",
      "LBFGS Iteration 350, Loss: 1767443.75\n",
      "LBFGS Iteration 360, Loss: 1766783.0\n",
      "LBFGS Iteration 370, Loss: 1766228.875\n",
      "LBFGS Iteration 380, Loss: 1764950.25\n",
      "LBFGS Iteration 390, Loss: 1763788.875\n",
      "LBFGS Iteration 400, Loss: 1762605.25\n",
      "LBFGS Iteration 410, Loss: 1761527.375\n",
      "LBFGS Iteration 420, Loss: 1759816.25\n",
      "LBFGS Iteration 430, Loss: 1758706.5\n",
      "LBFGS Iteration 440, Loss: 1757572.75\n",
      "LBFGS Iteration 450, Loss: 1755686.25\n",
      "LBFGS Iteration 460, Loss: 1754792.75\n",
      "LBFGS Iteration 470, Loss: 1754354.125\n",
      "LBFGS Iteration 480, Loss: 1753985.0\n",
      "LBFGS Iteration 490, Loss: 1752859.125\n",
      "LBFGS Iteration 500, Loss: 1752108.75\n",
      "LBFGS Iteration 510, Loss: 1751343.375\n",
      "LBFGS Iteration 520, Loss: 1751008.125\n",
      "LBFGS Iteration 530, Loss: 1750853.375\n",
      "LBFGS Iteration 540, Loss: 1750663.875\n",
      "LBFGS Iteration 550, Loss: 1750289.125\n",
      "LBFGS Iteration 560, Loss: 1749262.375\n",
      "LBFGS Iteration 570, Loss: 1748793.625\n",
      "LBFGS Iteration 580, Loss: 1748611.875\n",
      "LBFGS Iteration 590, Loss: 1748452.375\n",
      "LBFGS Iteration 600, Loss: 1748326.875\n",
      "LBFGS Iteration 610, Loss: 1748171.125\n",
      "LBFGS Iteration 620, Loss: 1747960.375\n",
      "LBFGS Iteration 630, Loss: 1747750.875\n",
      "LBFGS Iteration 640, Loss: 1747447.125\n",
      "LBFGS Iteration 650, Loss: 1746900.375\n",
      "LBFGS Iteration 660, Loss: 1746026.5\n",
      "LBFGS Iteration 670, Loss: 1745154.625\n",
      "LBFGS Iteration 680, Loss: 1744585.375\n",
      "LBFGS Iteration 690, Loss: 1744393.625\n",
      "LBFGS Iteration 700, Loss: 1744086.0\n",
      "LBFGS Iteration 710, Loss: 1743693.625\n",
      "LBFGS Iteration 720, Loss: 1742815.5\n",
      "LBFGS Iteration 730, Loss: 1741869.625\n",
      "LBFGS Iteration 740, Loss: 1741747.5\n",
      "LBFGS Iteration 750, Loss: 1741168.25\n",
      "LBFGS Iteration 760, Loss: 1740089.25\n",
      "LBFGS Iteration 770, Loss: 1739579.875\n",
      "LBFGS Iteration 780, Loss: 1739246.875\n",
      "LBFGS Iteration 790, Loss: 1739070.125\n",
      "LBFGS Iteration 800, Loss: 1738841.625\n",
      "LBFGS Iteration 810, Loss: 1738639.375\n",
      "LBFGS Iteration 820, Loss: 1738213.0\n",
      "LBFGS Iteration 830, Loss: 1737855.375\n",
      "LBFGS Iteration 840, Loss: 1737560.875\n",
      "LBFGS Iteration 850, Loss: 1737073.875\n",
      "LBFGS Iteration 860, Loss: 1736763.0\n",
      "LBFGS Iteration 870, Loss: 1736571.125\n",
      "LBFGS Iteration 880, Loss: 1736391.375\n",
      "LBFGS Iteration 890, Loss: 1736263.625\n",
      "LBFGS Iteration 900, Loss: 1736152.625\n",
      "LBFGS Iteration 910, Loss: 1735695.125\n",
      "LBFGS Iteration 920, Loss: 1735261.625\n",
      "LBFGS Iteration 930, Loss: 1734800.625\n",
      "LBFGS Iteration 940, Loss: 1734315.125\n",
      "LBFGS Iteration 950, Loss: 1733790.625\n",
      "LBFGS Iteration 960, Loss: 1733343.875\n",
      "LBFGS Iteration 970, Loss: 1733106.125\n",
      "LBFGS Iteration 980, Loss: 1733025.625\n",
      "LBFGS Iteration 990, Loss: 1732864.625\n",
      "LBFGS Iteration 1000, Loss: 1732426.75\n",
      "LBFGS Iteration 1010, Loss: 1732200.375\n",
      "LBFGS Iteration 1020, Loss: 1732089.5\n",
      "LBFGS Iteration 1030, Loss: 1731530.5\n",
      "LBFGS Iteration 1040, Loss: 1730971.125\n",
      "LBFGS Iteration 1050, Loss: 1730361.625\n",
      "LBFGS Iteration 1060, Loss: 1730121.75\n",
      "LBFGS Iteration 1070, Loss: 1729777.625\n",
      "LBFGS Iteration 1080, Loss: 1729008.875\n",
      "LBFGS Iteration 1090, Loss: 1728236.625\n",
      "LBFGS Iteration 1100, Loss: 1727715.125\n",
      "LBFGS Iteration 1110, Loss: 1727370.25\n",
      "LBFGS Iteration 1120, Loss: 1727032.0\n",
      "LBFGS Iteration 1130, Loss: 1726673.0\n",
      "LBFGS Iteration 1140, Loss: 1726563.875\n",
      "LBFGS Iteration 1150, Loss: 1726467.375\n",
      "LBFGS Iteration 1160, Loss: 1726152.875\n",
      "LBFGS Iteration 1170, Loss: 1725782.125\n",
      "LBFGS Iteration 1180, Loss: 1725571.0\n",
      "LBFGS Iteration 1190, Loss: 1725027.875\n",
      "LBFGS Iteration 1200, Loss: 1724675.625\n",
      "LBFGS Iteration 1210, Loss: 1724304.5\n",
      "LBFGS Iteration 1220, Loss: 1723908.375\n",
      "LBFGS Iteration 1230, Loss: 1723425.5\n",
      "LBFGS Iteration 1240, Loss: 1723146.625\n",
      "LBFGS Iteration 1250, Loss: 1722939.0\n",
      "LBFGS Iteration 1260, Loss: 1722638.5\n",
      "LBFGS Iteration 1270, Loss: 1722545.25\n",
      "LBFGS Iteration 1280, Loss: 1722398.125\n",
      "LBFGS Iteration 1290, Loss: 1721932.625\n",
      "LBFGS Iteration 1300, Loss: 1721582.625\n",
      "LBFGS Iteration 1310, Loss: 1721250.25\n",
      "LBFGS Iteration 1320, Loss: 1720595.375\n",
      "LBFGS Iteration 1330, Loss: 1720373.25\n",
      "LBFGS Iteration 1340, Loss: 1720067.25\n",
      "LBFGS Iteration 1350, Loss: 1719597.0\n",
      "LBFGS Iteration 1360, Loss: 1719078.5\n",
      "LBFGS Iteration 1370, Loss: 1718993.625\n",
      "LBFGS Iteration 1380, Loss: 1718854.75\n",
      "LBFGS Iteration 1390, Loss: 1718778.25\n",
      "LBFGS Iteration 1400, Loss: 1718184.625\n",
      "LBFGS Iteration 1410, Loss: 1717529.125\n",
      "LBFGS Iteration 1420, Loss: 1717235.5\n",
      "LBFGS Iteration 1430, Loss: 1717063.125\n",
      "LBFGS Iteration 1440, Loss: 1716799.625\n",
      "LBFGS Iteration 1450, Loss: 1716620.875\n",
      "LBFGS Iteration 1460, Loss: 1716203.125\n",
      "LBFGS Iteration 1470, Loss: 1715405.625\n",
      "LBFGS Iteration 1480, Loss: 1715255.25\n",
      "LBFGS Iteration 1490, Loss: 1715004.375\n",
      "LBFGS Iteration 1500, Loss: 1714452.5\n",
      "LBFGS Iteration 1510, Loss: 1713141.75\n",
      "LBFGS Iteration 1520, Loss: 1712810.5\n",
      "LBFGS Iteration 1530, Loss: 1712631.375\n",
      "LBFGS Iteration 1540, Loss: 1712428.125\n",
      "LBFGS Iteration 1550, Loss: 1711700.25\n",
      "LBFGS Iteration 1560, Loss: 1711187.375\n",
      "LBFGS Iteration 1570, Loss: 1711037.375\n",
      "LBFGS Iteration 1580, Loss: 1710692.625\n",
      "LBFGS Iteration 1590, Loss: 1710237.375\n",
      "LBFGS Iteration 1600, Loss: 1709960.125\n",
      "LBFGS Iteration 1610, Loss: 1709569.125\n",
      "LBFGS Iteration 1620, Loss: 1709290.75\n",
      "LBFGS Iteration 1630, Loss: 1709131.75\n",
      "LBFGS Iteration 1640, Loss: 1708913.25\n",
      "LBFGS Iteration 1650, Loss: 1708515.5\n",
      "LBFGS Iteration 1660, Loss: 1707809.625\n",
      "LBFGS Iteration 1670, Loss: 1707265.75\n",
      "LBFGS Iteration 1680, Loss: 1707064.75\n",
      "LBFGS Iteration 1690, Loss: 1706793.5\n",
      "LBFGS Iteration 1700, Loss: 1706626.125\n",
      "LBFGS Iteration 1710, Loss: 1707033.125\n",
      "LBFGS Iteration 1720, Loss: 1705700.25\n",
      "LBFGS Iteration 1730, Loss: 1704955.0\n",
      "LBFGS Iteration 1740, Loss: 1704363.875\n",
      "LBFGS Iteration 1750, Loss: 1704070.5\n",
      "LBFGS Iteration 1760, Loss: 1703781.0\n",
      "LBFGS Iteration 1770, Loss: 1703579.25\n",
      "LBFGS Iteration 1780, Loss: 1703354.125\n",
      "LBFGS Iteration 1790, Loss: 1703049.375\n",
      "LBFGS Iteration 1800, Loss: 1702256.25\n",
      "LBFGS Iteration 1810, Loss: 1701973.875\n",
      "LBFGS Iteration 1820, Loss: 1701858.875\n",
      "LBFGS Iteration 1830, Loss: 1701454.125\n",
      "LBFGS Iteration 1840, Loss: 1700957.25\n",
      "LBFGS Iteration 1850, Loss: 1700725.875\n",
      "LBFGS Iteration 1860, Loss: 1700425.75\n",
      "LBFGS Iteration 1870, Loss: 1700080.875\n",
      "LBFGS Iteration 1880, Loss: 1699693.25\n",
      "LBFGS Iteration 1890, Loss: 1698989.0\n",
      "LBFGS Iteration 1900, Loss: 1698557.0\n",
      "LBFGS Iteration 1910, Loss: 1698400.75\n",
      "LBFGS Iteration 1920, Loss: 1698285.125\n",
      "LBFGS Iteration 1930, Loss: 1698005.125\n",
      "LBFGS Iteration 1940, Loss: 1697787.875\n",
      "LBFGS Iteration 1950, Loss: 1697453.75\n",
      "LBFGS Iteration 1960, Loss: 1697151.75\n",
      "LBFGS Iteration 1970, Loss: 1696767.5\n",
      "LBFGS Iteration 1980, Loss: 1696161.375\n",
      "LBFGS Iteration 1990, Loss: 1695930.875\n",
      "LBFGS Iteration 2000, Loss: 1695782.75\n",
      "LBFGS Iteration 2010, Loss: 1695640.75\n",
      "LBFGS Iteration 2020, Loss: 1695490.875\n",
      "LBFGS Iteration 2030, Loss: 1695353.125\n",
      "LBFGS Iteration 2040, Loss: 1695217.125\n",
      "LBFGS Iteration 2050, Loss: 1695097.625\n",
      "LBFGS Iteration 2060, Loss: 1694999.0\n",
      "LBFGS Iteration 2070, Loss: 1694842.5\n",
      "LBFGS Iteration 2080, Loss: 1694334.375\n",
      "LBFGS Iteration 2090, Loss: 1694104.5\n",
      "LBFGS Iteration 2100, Loss: 1694016.75\n",
      "LBFGS Iteration 2110, Loss: 1693969.75\n",
      "LBFGS Iteration 2120, Loss: 1693925.75\n",
      "LBFGS Iteration 2130, Loss: 1693838.875\n",
      "LBFGS Iteration 2140, Loss: 1693635.75\n",
      "LBFGS Iteration 2150, Loss: 1693303.25\n",
      "LBFGS Iteration 2160, Loss: 1693193.375\n",
      "LBFGS Iteration 2170, Loss: 1693016.375\n",
      "LBFGS Iteration 2180, Loss: 1692916.875\n",
      "LBFGS Iteration 2190, Loss: 1692795.875\n",
      "LBFGS Iteration 2200, Loss: 1692517.75\n",
      "LBFGS Iteration 2210, Loss: 1692219.5\n",
      "LBFGS Iteration 2220, Loss: 1691917.75\n",
      "LBFGS Iteration 2230, Loss: 1691572.25\n",
      "LBFGS Iteration 2240, Loss: 1691390.5\n",
      "LBFGS Iteration 2250, Loss: 1691151.375\n",
      "LBFGS Iteration 2260, Loss: 1690845.5\n",
      "LBFGS Iteration 2270, Loss: 1690623.75\n",
      "LBFGS Iteration 2280, Loss: 1690403.125\n",
      "LBFGS Iteration 2290, Loss: 1690243.25\n",
      "LBFGS Iteration 2300, Loss: 1689935.625\n",
      "LBFGS Iteration 2310, Loss: 1689442.75\n",
      "LBFGS Iteration 2320, Loss: 1689238.125\n",
      "LBFGS Iteration 2330, Loss: 1688690.75\n",
      "LBFGS Iteration 2340, Loss: 1688120.5\n",
      "LBFGS Iteration 2350, Loss: 1687859.75\n",
      "LBFGS Iteration 2360, Loss: 1687643.0\n",
      "LBFGS Iteration 2370, Loss: 1687338.75\n",
      "LBFGS Iteration 2380, Loss: 1686790.875\n",
      "LBFGS Iteration 2390, Loss: 1686331.125\n",
      "LBFGS Iteration 2400, Loss: 1686149.875\n",
      "LBFGS Iteration 2410, Loss: 1685635.0\n",
      "LBFGS Iteration 2420, Loss: 1684411.0\n",
      "LBFGS Iteration 2430, Loss: 1684181.375\n",
      "LBFGS Iteration 2440, Loss: 1683917.25\n",
      "LBFGS Iteration 2450, Loss: 1683523.25\n",
      "LBFGS Iteration 2460, Loss: 1683152.125\n",
      "LBFGS Iteration 2470, Loss: 1683036.5\n",
      "LBFGS Iteration 2480, Loss: 1682861.0\n",
      "LBFGS Iteration 2490, Loss: 1682703.5\n",
      "LBFGS Iteration 2500, Loss: 1682501.25\n",
      "LBFGS Iteration 2510, Loss: 1682163.25\n",
      "LBFGS Iteration 2520, Loss: 1682001.25\n",
      "LBFGS Iteration 2530, Loss: 1681822.625\n",
      "LBFGS Iteration 2540, Loss: 1681493.75\n",
      "LBFGS Iteration 2550, Loss: 1681325.125\n",
      "LBFGS Iteration 2560, Loss: 1681021.0\n",
      "LBFGS Iteration 2570, Loss: 1680463.875\n",
      "LBFGS Iteration 2580, Loss: 1680085.25\n",
      "LBFGS Iteration 2590, Loss: 1679707.25\n",
      "LBFGS Iteration 2600, Loss: 1679318.75\n",
      "LBFGS Iteration 2610, Loss: 1679079.625\n",
      "LBFGS Iteration 2620, Loss: 1678811.75\n",
      "LBFGS Iteration 2630, Loss: 1678516.0\n",
      "LBFGS Iteration 2640, Loss: 1678384.5\n",
      "LBFGS Iteration 2650, Loss: 1678177.25\n",
      "LBFGS Iteration 2660, Loss: 1677793.625\n",
      "LBFGS Iteration 2670, Loss: 1677570.0\n",
      "LBFGS Iteration 2680, Loss: 1677316.875\n",
      "LBFGS Iteration 2690, Loss: 1677117.25\n",
      "LBFGS Iteration 2700, Loss: 1676597.25\n",
      "LBFGS Iteration 2710, Loss: 1676241.75\n",
      "LBFGS Iteration 2720, Loss: 1676000.5\n",
      "LBFGS Iteration 2730, Loss: 1675860.5\n",
      "LBFGS Iteration 2740, Loss: 1675475.0\n",
      "LBFGS Iteration 2750, Loss: 1674779.875\n",
      "LBFGS Iteration 2760, Loss: 1674052.5\n",
      "LBFGS Iteration 2770, Loss: 1673897.0\n",
      "LBFGS Iteration 2780, Loss: 1673722.75\n",
      "LBFGS Iteration 2790, Loss: 1672843.25\n",
      "LBFGS Iteration 2800, Loss: 1672265.0\n",
      "LBFGS Iteration 2810, Loss: 1671859.875\n",
      "LBFGS Iteration 2820, Loss: 1671567.375\n",
      "LBFGS Iteration 2830, Loss: 1671503.125\n",
      "LBFGS Iteration 2840, Loss: 1671442.25\n",
      "LBFGS Iteration 2850, Loss: 1671391.25\n",
      "LBFGS Iteration 2860, Loss: 1671256.25\n",
      "LBFGS Iteration 2870, Loss: 1670849.25\n",
      "LBFGS Iteration 2880, Loss: 1670319.0\n",
      "LBFGS Iteration 2890, Loss: 1669966.25\n",
      "LBFGS Iteration 2900, Loss: 1669819.625\n",
      "LBFGS Iteration 2910, Loss: 1669698.25\n",
      "LBFGS Iteration 2920, Loss: 1669485.5\n",
      "LBFGS Iteration 2930, Loss: 1669321.875\n",
      "LBFGS Iteration 2940, Loss: 1669185.125\n",
      "LBFGS Iteration 2950, Loss: 1669092.25\n",
      "LBFGS Iteration 2960, Loss: 1669024.25\n",
      "LBFGS Iteration 2970, Loss: 1668934.375\n",
      "LBFGS Iteration 2980, Loss: 1668807.125\n",
      "LBFGS Iteration 2990, Loss: 1668523.75\n",
      "LBFGS Iteration 3000, Loss: 1668408.375\n",
      "LBFGS Iteration 3010, Loss: 1668301.125\n",
      "LBFGS Iteration 3020, Loss: 1668151.875\n",
      "LBFGS Iteration 3030, Loss: 1668106.125\n",
      "LBFGS Iteration 3040, Loss: 1667967.625\n",
      "LBFGS Iteration 3050, Loss: 1667820.0\n",
      "LBFGS Iteration 3060, Loss: 1667901.5\n",
      "LBFGS Iteration 3070, Loss: 1667404.0\n",
      "LBFGS Iteration 3080, Loss: 1667037.875\n",
      "LBFGS Iteration 3090, Loss: 1666683.0\n",
      "LBFGS Iteration 3100, Loss: 1666016.25\n",
      "LBFGS Iteration 3110, Loss: 1665836.25\n",
      "LBFGS Iteration 3120, Loss: 1665698.875\n",
      "LBFGS Iteration 3130, Loss: 1665477.875\n",
      "LBFGS Iteration 3140, Loss: 1665254.375\n",
      "LBFGS Iteration 3150, Loss: 1665013.0\n",
      "LBFGS Iteration 3160, Loss: 1664803.25\n",
      "LBFGS Iteration 3170, Loss: 1664620.25\n",
      "LBFGS Iteration 3180, Loss: 1663804.625\n",
      "LBFGS Iteration 3190, Loss: 1663039.875\n",
      "LBFGS Iteration 3200, Loss: 1662677.0\n",
      "LBFGS Iteration 3210, Loss: 1662570.625\n",
      "LBFGS Iteration 3220, Loss: 1662495.0\n",
      "LBFGS Iteration 3230, Loss: 1662328.375\n",
      "LBFGS Iteration 3240, Loss: 1662095.125\n",
      "LBFGS Iteration 3250, Loss: 1662021.625\n",
      "LBFGS Iteration 3260, Loss: 1661923.5\n",
      "LBFGS Iteration 3270, Loss: 1661718.125\n",
      "LBFGS Iteration 3280, Loss: 1661463.125\n",
      "LBFGS Iteration 3290, Loss: 1661277.625\n",
      "LBFGS Iteration 3300, Loss: 1660965.75\n",
      "LBFGS Iteration 3310, Loss: 1660477.0\n",
      "LBFGS Iteration 3320, Loss: 1660077.375\n",
      "LBFGS Iteration 3330, Loss: 1659681.125\n",
      "LBFGS Iteration 3340, Loss: 1659399.125\n",
      "LBFGS Iteration 3350, Loss: 1659120.5\n",
      "LBFGS Iteration 3360, Loss: 1659050.0\n",
      "LBFGS Iteration 3370, Loss: 1658865.375\n",
      "LBFGS Iteration 3380, Loss: 1658726.5\n",
      "LBFGS Iteration 3390, Loss: 1658437.25\n",
      "LBFGS Iteration 3400, Loss: 1657904.5\n",
      "LBFGS Iteration 3410, Loss: 1657520.875\n",
      "LBFGS Iteration 3420, Loss: 1657362.75\n",
      "LBFGS Iteration 3430, Loss: 1657232.625\n",
      "LBFGS Iteration 3440, Loss: 1657095.375\n",
      "LBFGS Iteration 3450, Loss: 1656842.25\n",
      "LBFGS Iteration 3460, Loss: 1656537.625\n",
      "LBFGS Iteration 3470, Loss: 1656301.0\n",
      "LBFGS Iteration 3480, Loss: 1656000.875\n",
      "LBFGS Iteration 3490, Loss: 1655839.375\n",
      "LBFGS Iteration 3500, Loss: 1655685.0\n",
      "LBFGS Iteration 3510, Loss: 1655528.375\n",
      "LBFGS Iteration 3520, Loss: 1655355.5\n",
      "LBFGS Iteration 3530, Loss: 1655272.625\n",
      "LBFGS Iteration 3540, Loss: 1655198.125\n",
      "LBFGS Iteration 3550, Loss: 1655081.5\n",
      "LBFGS Iteration 3560, Loss: 1654807.0\n",
      "LBFGS Iteration 3570, Loss: 1654646.25\n",
      "LBFGS Iteration 3580, Loss: 1654398.125\n",
      "LBFGS Iteration 3590, Loss: 1654126.0\n",
      "LBFGS Iteration 3600, Loss: 1653981.625\n",
      "LBFGS Iteration 3610, Loss: 1653864.5\n",
      "LBFGS Iteration 3620, Loss: 1653649.0\n",
      "LBFGS Iteration 3630, Loss: 1653421.75\n",
      "LBFGS Iteration 3640, Loss: 1653333.0\n",
      "LBFGS Iteration 3650, Loss: 1653196.125\n",
      "LBFGS Iteration 3660, Loss: 1652945.375\n",
      "LBFGS Iteration 3670, Loss: 1652781.75\n",
      "LBFGS Iteration 3680, Loss: 1652637.75\n",
      "LBFGS Iteration 3690, Loss: 1652550.375\n",
      "LBFGS Iteration 3700, Loss: 1651995.0\n",
      "LBFGS Iteration 3710, Loss: 1651547.625\n",
      "LBFGS Iteration 3720, Loss: 1651453.25\n",
      "LBFGS Iteration 3730, Loss: 1651351.25\n",
      "LBFGS Iteration 3740, Loss: 1651111.75\n",
      "LBFGS Iteration 3750, Loss: 1650337.875\n",
      "LBFGS Iteration 3760, Loss: 1649624.25\n",
      "LBFGS Iteration 3770, Loss: 1649238.0\n",
      "LBFGS Iteration 3780, Loss: 1649091.0\n",
      "LBFGS Iteration 3790, Loss: 1649028.25\n",
      "LBFGS Iteration 3800, Loss: 1648963.5\n",
      "LBFGS Iteration 3810, Loss: 1648840.75\n",
      "LBFGS Iteration 3820, Loss: 1648592.5\n",
      "LBFGS Iteration 3830, Loss: 1648329.5\n",
      "LBFGS Iteration 3840, Loss: 1648215.75\n",
      "LBFGS Iteration 3850, Loss: 1648099.125\n",
      "LBFGS Iteration 3860, Loss: 1647941.625\n",
      "LBFGS Iteration 3870, Loss: 1647456.875\n",
      "LBFGS Iteration 3880, Loss: 1647004.5\n",
      "LBFGS Iteration 3890, Loss: 1646883.25\n",
      "LBFGS Iteration 3900, Loss: 1646787.125\n",
      "LBFGS Iteration 3910, Loss: 1646736.875\n",
      "LBFGS Iteration 3920, Loss: 1646697.0\n",
      "LBFGS Iteration 3930, Loss: 1646623.25\n",
      "LBFGS Iteration 3940, Loss: 1646542.25\n",
      "LBFGS Iteration 3950, Loss: 1646523.25\n",
      "LBFGS Iteration 3960, Loss: 1646476.875\n",
      "LBFGS Iteration 3970, Loss: 1646410.75\n",
      "LBFGS Iteration 3980, Loss: 1646311.25\n",
      "LBFGS Iteration 3990, Loss: 1646167.625\n",
      "LBFGS Iteration 4000, Loss: 1646064.375\n",
      "LBFGS Iteration 4010, Loss: 1645958.625\n",
      "LBFGS Iteration 4020, Loss: 1645810.375\n",
      "LBFGS Iteration 4030, Loss: 1645653.25\n",
      "LBFGS Iteration 4040, Loss: 1645377.375\n",
      "LBFGS Iteration 4050, Loss: 1645243.625\n",
      "LBFGS Iteration 4060, Loss: 1644918.375\n",
      "LBFGS Iteration 4070, Loss: 1644508.0\n",
      "LBFGS Iteration 4080, Loss: 1644303.125\n",
      "LBFGS Iteration 4090, Loss: 1644022.5\n",
      "LBFGS Iteration 4100, Loss: 1643847.625\n",
      "LBFGS Iteration 4110, Loss: 1643751.0\n",
      "LBFGS Iteration 4120, Loss: 1643621.0\n",
      "LBFGS Iteration 4130, Loss: 1643495.5\n",
      "LBFGS Iteration 4140, Loss: 1643324.375\n",
      "LBFGS Iteration 4150, Loss: 1643192.625\n",
      "LBFGS Iteration 4160, Loss: 1643122.125\n",
      "LBFGS Iteration 4170, Loss: 1642999.0\n",
      "LBFGS Iteration 4180, Loss: 1642802.875\n",
      "LBFGS Iteration 4190, Loss: 1642664.75\n",
      "LBFGS Iteration 4200, Loss: 1642105.0\n",
      "LBFGS Iteration 4210, Loss: 1641545.25\n",
      "LBFGS Iteration 4220, Loss: 1641365.875\n",
      "LBFGS Iteration 4230, Loss: 1640974.5\n",
      "LBFGS Iteration 4240, Loss: 1640557.5\n",
      "LBFGS Iteration 4250, Loss: 1640474.375\n",
      "LBFGS Iteration 4260, Loss: 1640202.75\n",
      "LBFGS Iteration 4270, Loss: 1640126.625\n",
      "LBFGS Iteration 4280, Loss: 1640014.875\n",
      "LBFGS Iteration 4290, Loss: 1639922.875\n",
      "LBFGS Iteration 4300, Loss: 1639811.875\n",
      "LBFGS Iteration 4310, Loss: 1639626.125\n",
      "LBFGS Iteration 4320, Loss: 1639483.25\n",
      "LBFGS Iteration 4330, Loss: 1639330.5\n",
      "LBFGS Iteration 4340, Loss: 1639222.125\n",
      "LBFGS Iteration 4350, Loss: 1639096.5\n",
      "LBFGS Iteration 4360, Loss: 1638890.375\n",
      "LBFGS Iteration 4370, Loss: 1638569.25\n",
      "LBFGS Iteration 4380, Loss: 1638330.875\n",
      "LBFGS Iteration 4390, Loss: 1638301.5\n",
      "LBFGS Iteration 4400, Loss: 1638261.5\n",
      "LBFGS Iteration 4410, Loss: 1638142.5\n",
      "LBFGS Iteration 4420, Loss: 1638047.625\n",
      "LBFGS Iteration 4430, Loss: 1638012.0\n",
      "LBFGS Iteration 4440, Loss: 1637940.875\n",
      "LBFGS Iteration 4450, Loss: 1637753.25\n",
      "LBFGS Iteration 4460, Loss: 1637462.875\n",
      "LBFGS Iteration 4470, Loss: 1637241.0\n",
      "LBFGS Iteration 4480, Loss: 1637061.875\n",
      "LBFGS Iteration 4490, Loss: 1636859.375\n",
      "LBFGS Iteration 4500, Loss: 1636718.0\n",
      "LBFGS Iteration 4510, Loss: 1636490.5\n",
      "LBFGS Iteration 4520, Loss: 1636081.125\n",
      "LBFGS Iteration 4530, Loss: 1635527.0\n",
      "LBFGS Iteration 4540, Loss: 1635156.25\n",
      "LBFGS Iteration 4550, Loss: 1634897.875\n",
      "LBFGS Iteration 4560, Loss: 1634648.0\n",
      "LBFGS Iteration 4570, Loss: 1634469.5\n",
      "LBFGS Iteration 4580, Loss: 1634364.25\n",
      "LBFGS Iteration 4590, Loss: 1634192.5\n",
      "LBFGS Iteration 4600, Loss: 1633976.25\n",
      "LBFGS Iteration 4610, Loss: 1633681.375\n",
      "LBFGS Iteration 4620, Loss: 1633535.5\n",
      "LBFGS Iteration 4630, Loss: 1633447.5\n",
      "LBFGS Iteration 4640, Loss: 1633385.0\n",
      "LBFGS Iteration 4650, Loss: 1633322.375\n",
      "LBFGS Iteration 4660, Loss: 1633144.625\n",
      "LBFGS Iteration 4670, Loss: 1632950.25\n",
      "LBFGS Iteration 4680, Loss: 1632905.25\n",
      "LBFGS Iteration 4690, Loss: 1632828.5\n",
      "LBFGS Iteration 4700, Loss: 1632741.5\n",
      "LBFGS Iteration 4710, Loss: 1632679.5\n",
      "LBFGS Iteration 4720, Loss: 1632529.5\n",
      "LBFGS Iteration 4730, Loss: 1632247.5\n",
      "LBFGS Iteration 4740, Loss: 1632082.25\n",
      "LBFGS Iteration 4750, Loss: 1631854.25\n",
      "LBFGS Iteration 4760, Loss: 1631679.25\n",
      "LBFGS Iteration 4770, Loss: 1631578.5\n",
      "LBFGS Iteration 4780, Loss: 1631431.125\n",
      "LBFGS Iteration 4790, Loss: 1631340.5\n",
      "LBFGS Iteration 4800, Loss: 1631149.625\n",
      "LBFGS Iteration 4810, Loss: 1630818.125\n",
      "LBFGS Iteration 4820, Loss: 1630795.625\n",
      "LBFGS Iteration 4830, Loss: 1630767.375\n",
      "LBFGS Iteration 4840, Loss: 1630674.0\n",
      "LBFGS Iteration 4850, Loss: 1630392.375\n",
      "LBFGS Iteration 4860, Loss: 1630257.5\n",
      "LBFGS Iteration 4870, Loss: 1630214.125\n",
      "LBFGS Iteration 4880, Loss: 1630031.375\n",
      "LBFGS Iteration 4890, Loss: 1629813.75\n",
      "LBFGS Iteration 4900, Loss: 1629720.0\n",
      "LBFGS Iteration 4910, Loss: 1629652.375\n",
      "LBFGS Iteration 4920, Loss: 1629571.875\n",
      "LBFGS Iteration 4930, Loss: 1629439.125\n",
      "LBFGS Iteration 4940, Loss: 1629337.25\n",
      "LBFGS Iteration 4950, Loss: 1629213.375\n",
      "LBFGS Iteration 4960, Loss: 1629174.375\n",
      "LBFGS Iteration 4970, Loss: 1629126.5\n",
      "LBFGS Iteration 4980, Loss: 1628924.25\n",
      "LBFGS Iteration 4990, Loss: 1628537.625\n",
      "LBFGS Iteration 5000, Loss: 1628002.5\n",
      "LBFGS Iteration 5010, Loss: 1627708.875\n",
      "LBFGS Iteration 5020, Loss: 1627373.625\n",
      "LBFGS Iteration 5030, Loss: 1627158.5\n",
      "LBFGS Iteration 5040, Loss: 1626934.5\n",
      "LBFGS Iteration 5050, Loss: 1626770.0\n",
      "LBFGS Iteration 5060, Loss: 1626693.75\n",
      "LBFGS Iteration 5070, Loss: 1626530.5\n",
      "LBFGS Iteration 5080, Loss: 1626277.375\n",
      "LBFGS Iteration 5090, Loss: 1626026.875\n",
      "LBFGS Iteration 5100, Loss: 1625894.75\n",
      "LBFGS Iteration 5110, Loss: 1625702.125\n",
      "LBFGS Iteration 5120, Loss: 1625447.75\n",
      "LBFGS Iteration 5130, Loss: 1625043.75\n",
      "LBFGS Iteration 5140, Loss: 1624684.875\n",
      "LBFGS Iteration 5150, Loss: 1624660.25\n",
      "LBFGS Iteration 5160, Loss: 1624537.25\n",
      "LBFGS Iteration 5170, Loss: 1624107.625\n",
      "LBFGS Iteration 5180, Loss: 1623236.625\n",
      "LBFGS Iteration 5190, Loss: 1622840.25\n",
      "LBFGS Iteration 5200, Loss: 1622691.875\n",
      "LBFGS Iteration 5210, Loss: 1622560.75\n",
      "LBFGS Iteration 5220, Loss: 1622312.375\n",
      "LBFGS Iteration 5230, Loss: 1622175.5\n",
      "LBFGS Iteration 5240, Loss: 1622082.375\n",
      "LBFGS Iteration 5250, Loss: 1621931.75\n",
      "LBFGS Iteration 5260, Loss: 1621788.375\n",
      "LBFGS Iteration 5270, Loss: 1621698.0\n",
      "LBFGS Iteration 5280, Loss: 1621647.375\n",
      "LBFGS Iteration 5290, Loss: 1621594.5\n",
      "LBFGS Iteration 5300, Loss: 1621501.625\n",
      "LBFGS Iteration 5310, Loss: 1621385.0\n",
      "LBFGS Iteration 5320, Loss: 1621255.0\n",
      "LBFGS Iteration 5330, Loss: 1621120.875\n",
      "LBFGS Iteration 5340, Loss: 1620987.5\n",
      "LBFGS Iteration 5350, Loss: 1620801.75\n",
      "LBFGS Iteration 5360, Loss: 1620742.375\n",
      "LBFGS Iteration 5370, Loss: 1620608.625\n",
      "LBFGS Iteration 5380, Loss: 1620466.5\n",
      "LBFGS Iteration 5390, Loss: 1620213.125\n",
      "LBFGS Iteration 5400, Loss: 1620021.5\n",
      "LBFGS Iteration 5410, Loss: 1619874.625\n",
      "LBFGS Iteration 5420, Loss: 1619642.125\n",
      "LBFGS Iteration 5430, Loss: 1619533.875\n",
      "LBFGS Iteration 5440, Loss: 1619391.5\n",
      "LBFGS Iteration 5450, Loss: 1619181.5\n",
      "LBFGS Iteration 5460, Loss: 1618983.25\n",
      "LBFGS Iteration 5470, Loss: 1618883.25\n",
      "LBFGS Iteration 5480, Loss: 1618834.625\n",
      "LBFGS Iteration 5490, Loss: 1618774.375\n",
      "LBFGS Iteration 5500, Loss: 1618705.75\n",
      "LBFGS Iteration 5510, Loss: 1618638.5\n",
      "LBFGS Iteration 5520, Loss: 1618593.375\n",
      "LBFGS Iteration 5530, Loss: 1618424.5\n",
      "LBFGS Iteration 5540, Loss: 1618222.5\n",
      "LBFGS Iteration 5550, Loss: 1618175.625\n",
      "LBFGS Iteration 5560, Loss: 1618163.375\n",
      "LBFGS Iteration 5570, Loss: 1618145.0\n",
      "LBFGS Iteration 5580, Loss: 1618118.5\n",
      "LBFGS Iteration 5590, Loss: 1618060.0\n",
      "LBFGS Iteration 5600, Loss: 1618001.75\n",
      "LBFGS Iteration 5610, Loss: 1617911.375\n",
      "LBFGS Iteration 5620, Loss: 1617828.375\n",
      "LBFGS Iteration 5630, Loss: 1617788.25\n",
      "LBFGS Iteration 5640, Loss: 1617750.125\n",
      "LBFGS Iteration 5650, Loss: 1617667.125\n",
      "LBFGS Iteration 5660, Loss: 1617626.25\n",
      "LBFGS Iteration 5670, Loss: 1617552.125\n",
      "LBFGS Iteration 5680, Loss: 1617430.25\n",
      "LBFGS Iteration 5690, Loss: 1617242.375\n",
      "LBFGS Iteration 5700, Loss: 1616886.625\n",
      "LBFGS Iteration 5710, Loss: 1616687.75\n",
      "LBFGS Iteration 5720, Loss: 1616584.25\n",
      "LBFGS Iteration 5730, Loss: 1616514.875\n",
      "LBFGS Iteration 5740, Loss: 1616447.0\n",
      "LBFGS Iteration 5750, Loss: 1616406.5\n",
      "LBFGS Iteration 5760, Loss: 1616359.375\n",
      "LBFGS Iteration 5770, Loss: 1616198.25\n",
      "LBFGS Iteration 5780, Loss: 1616079.125\n",
      "LBFGS Iteration 5790, Loss: 1616049.375\n",
      "LBFGS Iteration 5800, Loss: 1615986.5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model using Adam and LBFGS\n",
    "optimizer_adam = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer_lbfgs = torch.optim.LBFGS(net.parameters(), lr=0.1, max_iter=200000, max_eval=500000,\n",
    "                                    history_size=50, tolerance_grad=0.5 * np.finfo(float).eps, tolerance_change=0.5 * np.finfo(float).eps,\n",
    "                                    line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "loss_history = []\n",
    "lbfgs_iter = 0  # Initialize the LBFGS iteration counter\n",
    "patience = 10000  # Number of iterations with no significant improvement before stopping\n",
    "min_delta = 1e-7  # Minimum change to qualify as an improvement\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop with Adam\n",
    "# for epoch in range(100000):\n",
    "#     def closure():\n",
    "#         optimizer_adam.zero_grad()\n",
    "\n",
    "#         u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = create_pde(x_train, y_train, t_train)\n",
    "#         u_loss = mse(u_prediction, u_train)\n",
    "#         v_loss = mse(v_prediction, v_train)\n",
    "#         p_loss = mse(p_prediction, p_train)\n",
    "#         f_loss = mse(f_prediction, torch.zeros_like(f_prediction))\n",
    "#         g_loss = mse(g_prediction, torch.zeros_like(g_prediction))\n",
    "\n",
    "#         loss = u_loss + v_loss + p_loss + f_loss + g_loss\n",
    "#         loss.backward()\n",
    "#         return loss\n",
    "    \n",
    "#     optimizer_adam.step(closure)\n",
    "#     loss = closure().item()\n",
    "#     loss_history.append(loss)\n",
    "    \n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'Adam Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Fine-tuning with LBFGS and early stopping\n",
    "def closure():\n",
    "    global lbfgs_iter, best_loss, patience_counter  # Access the global iteration variables\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "\n",
    "    u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = create_pde(x_train, y_train, t_train)\n",
    "    u_loss = mse(u_prediction, u_train)\n",
    "    v_loss = mse(v_prediction, v_train)\n",
    "    p_loss = mse(p_prediction, p_train)\n",
    "    f_loss = mse(f_prediction, torch.zeros_like(f_prediction))\n",
    "    g_loss = mse(g_prediction, torch.zeros_like(g_prediction))\n",
    "\n",
    "    loss = u_loss + v_loss + p_loss + f_loss + g_loss\n",
    "    loss.backward()\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    # Print the current iteration and loss\n",
    "    lbfgs_iter += 1\n",
    "    if lbfgs_iter % 10 == 0:\n",
    "        print(f'LBFGS Iteration {lbfgs_iter}, Loss: {loss.item()}')\n",
    "\n",
    "    # Early stopping logic\n",
    "    # if loss.item() < best_loss - min_delta:\n",
    "    #     best_loss = loss.item()\n",
    "    #     patience_counter = 0\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         return loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "optimizer_lbfgs.step(closure)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), './models/20x100lyrs_NACA0012_extrapolate(0-140).pt')\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "np.savetxt(\"./Results/20x100lyrs_NACA0012_histloss.csv\", loss_history, delimiter=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "net = ResNet()\n",
    "state = torch.load('./models/20x100lyrs_NACA0012_extrapolate(0-140).pt')\n",
    "net.load_state_dict(state)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n",
      "/tmp/ipykernel_16886/599473574.py:31: MatplotlibDeprecationWarning: The collections attribute was deprecated in Matplotlib 3.8 and will be removed in 3.10.\n",
      "  for c in contour.collections:\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load the trained model and evaluate\n",
    "net.eval()\n",
    "\n",
    "x_test = torch.tensor(X_star[:, 0:1], dtype=torch.float32, requires_grad=True)\n",
    "y_test = torch.tensor(X_star[:, 1:2], dtype=torch.float32, requires_grad=True)\n",
    "t_test = torch.tensor(np.ones((X_star.shape[0], 1)), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "u_out, v_out, p_out, f_out, g_out = create_pde(x_test, y_test, t_test)\n",
    "\n",
    "\n",
    "# Function to create and save an animation for a specific field\n",
    "def create_animation(field_name, values_fn, vmin, vmax, filename):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Initialize the first frame\n",
    "    t = torch.tensor(np.full(x_test.shape, t_star[0]), dtype=torch.float32, requires_grad=True)\n",
    "    u_out, v_out, p_out, _, _ = create_pde(x_test, y_test, t)\n",
    "    field_values = values_fn(u_out, v_out, p_out).detach().cpu().numpy().flatten()\n",
    "    \n",
    "    contour = ax.tricontourf(x_test.detach().numpy().flatten(), y_test.detach().numpy().flatten(), field_values, levels=20, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    colorbar = fig.colorbar(contour, ax=ax, label=field_name)  # Add colorbar once\n",
    "\n",
    "    ax.set_xlim(-0.5, 8)  # Set x-axis limits\n",
    "    ax.set_ylim(-2, 2)  # Set y-axis limits\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'{field_name} at t = {t_star[0][0]}')\n",
    "\n",
    "    def animate(i):\n",
    "        nonlocal contour\n",
    "        for c in contour.collections:\n",
    "            c.remove()\n",
    "        t = torch.tensor(np.full(x_test.shape, t_star[i]), dtype=torch.float32, requires_grad=True)\n",
    "        u_out, v_out, p_out, _, _ = create_pde(x_test, y_test, t)\n",
    "        field_values = values_fn(u_out, v_out, p_out).detach().cpu().numpy().flatten()\n",
    "        contour = ax.tricontourf(x_test.detach().numpy().flatten(), y_test.detach().numpy().flatten(), field_values, levels=20, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f'{field_name} at t = {t_star[i][0]}')\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=T, interval=50, blit=False)\n",
    "    ani.save(filename, writer='imagemagick', fps=10)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Determine value ranges for proper color scaling\n",
    "t = torch.tensor(np.full(x_test.shape, t_star[0]), dtype=torch.float32, requires_grad=True)\n",
    "u_out, v_out, p_out, _, _ = create_pde(x_test, y_test, t)  # Extract only the first three values\n",
    "\n",
    "vmin_p, vmax_p = torch.min(p_out).item(), torch.max(p_out).item()\n",
    "vmin_u, vmax_u = torch.min(u_out).item(), torch.max(u_out).item()\n",
    "vmin_v, vmax_v = torch.min(v_out).item(), torch.max(v_out).item()\n",
    "\n",
    "# Create and save animations for each field\n",
    "create_animation('Pressure Field', lambda u_out, v_out, p_out: p_out, vmin_p, vmax_p, '0012_pressure_animation.gif')\n",
    "create_animation('U Velocity Field', lambda u_out, v_out, p_out: u_out, vmin_u, vmax_u, '0012_u_velocity_animation.gif')\n",
    "create_animation('V Velocity Field', lambda u_out, v_out, p_out: v_out, vmin_v, vmax_v, '0012_v_velocity_animation.gif')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
