{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "#nu = 0.00000016 # From very high Reynolds number\n",
    "nu = 0.01\n",
    "def create_pde(net, x, y, t):\n",
    "    res = net(torch.hstack((x, y, t)))\n",
    "    psi, p = res[:, 0:1], res[:, 1:2]\n",
    "\n",
    "    u = torch.autograd.grad(psi, y, grad_outputs=torch.ones_like(psi), create_graph=True)[0]  \n",
    "    v = -1. * torch.autograd.grad(psi, x, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
    "\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "\n",
    "    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "\n",
    "    f = u_t + u * u_x + v * u_y + p_x - nu * (u_xx + u_yy)\n",
    "    g = v_t + u * v_x + v * v_y + p_y - nu * (v_xx + v_yy)\n",
    "\n",
    "    return u, v, p, f, g\n",
    "\n",
    "# Initialize the network\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x)) + x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input_layer = nn.Linear(3, 100)\n",
    "        self.res_block1 = ResNetBlock(100, 100)\n",
    "        self.res_block2 = ResNetBlock(100, 100)\n",
    "        self.res_block3 = ResNetBlock(100, 100)\n",
    "        self.res_block4 = ResNetBlock(100, 100)\n",
    "        self.res_block5 = ResNetBlock(100, 100)\n",
    "        self.res_block6 = ResNetBlock(100, 100)\n",
    "        self.res_block7 = ResNetBlock(100, 100)\n",
    "        self.res_block8 = ResNetBlock(100, 100)\n",
    "        self.res_block9 = ResNetBlock(100, 100)\n",
    "        self.res_block10 = ResNetBlock(100, 100)\n",
    "        self.res_block11 = ResNetBlock(100, 100)\n",
    "        self.res_block12 = ResNetBlock(100, 100)\n",
    "        self.res_block13 = ResNetBlock(100, 100)\n",
    "        self.res_block14 = ResNetBlock(100, 100)\n",
    "        self.res_block15 = ResNetBlock(100, 100)\n",
    "        self.res_block16 = ResNetBlock(100, 100)\n",
    "        self.res_block17 = ResNetBlock(100, 100)\n",
    "        self.res_block18 = ResNetBlock(100, 100)\n",
    "        self.res_block19 = ResNetBlock(100, 100)\n",
    "        self.res_block20 = ResNetBlock(100, 100)\n",
    "        self.output_layer = nn.Linear(100, 3)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        x = self.res_block5(x)\n",
    "        x = self.res_block6(x)\n",
    "        x = self.res_block7(x)\n",
    "        x = self.res_block8(x)\n",
    "        x = self.res_block9(x)\n",
    "        x = self.res_block10(x)\n",
    "        x = self.res_block11(x)\n",
    "        x = self.res_block12(x)\n",
    "        x = self.res_block13(x)\n",
    "        x = self.res_block14(x)\n",
    "        x = self.res_block15(x)\n",
    "        x = self.res_block16(x)\n",
    "        x = self.res_block17(x)\n",
    "        x = self.res_block18(x)\n",
    "        x = self.res_block19(x)\n",
    "        x = self.res_block20(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "net = ResNet()\n",
    "state = torch.load('./models/20x100lyrs_NACA0012_extrapolate(0-140).pt')\n",
    "net.load_state_dict(state)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14449, 2, 140) (14449, 140) (140, 1) (14449, 2)\n",
      "Timestep 0 loss: 18821794496512.0\n",
      "Timestep 1 loss: 359579090944.0\n",
      "Timestep 2 loss: 5120379584512.0\n",
      "Timestep 3 loss: 135696695296.0\n",
      "Timestep 4 loss: 34304268288.0\n",
      "Timestep 5 loss: 275976832.0\n",
      "Timestep 6 loss: 2609897472.0\n",
      "Timestep 7 loss: 424550662144.0\n",
      "Timestep 8 loss: 34138253312.0\n",
      "Timestep 9 loss: 43623112704.0\n",
      "Timestep 10 loss: 54683716.0\n",
      "Timestep 11 loss: 8412302848.0\n",
      "Timestep 12 loss: 243248201728.0\n",
      "Timestep 13 loss: 199113637888.0\n",
      "Timestep 14 loss: 932379584.0\n",
      "Timestep 15 loss: 3688470151168.0\n",
      "Timestep 16 loss: 16273714970624.0\n",
      "Timestep 17 loss: 12295359954944.0\n",
      "Timestep 18 loss: 80639229952.0\n",
      "Timestep 19 loss: 7050674831360.0\n",
      "Timestep 20 loss: 1537556873216.0\n",
      "Timestep 21 loss: 2302650089472.0\n",
      "Timestep 22 loss: 4706911232.0\n",
      "Timestep 23 loss: 232109703168.0\n",
      "Timestep 24 loss: 4630586064896.0\n",
      "Timestep 25 loss: 475844050944.0\n",
      "Timestep 26 loss: 12214240256.0\n",
      "Timestep 27 loss: 697192939520.0\n",
      "Timestep 28 loss: 1146166837248.0\n",
      "Timestep 29 loss: 2494278926336.0\n",
      "Timestep 30 loss: 1741275922432.0\n",
      "Timestep 31 loss: 4744307277824.0\n",
      "Timestep 32 loss: 409346867200.0\n",
      "Timestep 33 loss: 29099200512.0\n",
      "Timestep 34 loss: 74205478912.0\n",
      "Timestep 35 loss: 109140467712.0\n",
      "Timestep 36 loss: 78516625408.0\n",
      "Timestep 37 loss: 627522207744.0\n",
      "Timestep 38 loss: 412664102912.0\n",
      "Timestep 39 loss: 2028831488.0\n",
      "Timestep 40 loss: 167839121408.0\n",
      "Timestep 41 loss: 221060005888.0\n",
      "Timestep 42 loss: 266095362048.0\n",
      "Timestep 43 loss: 412860022784.0\n",
      "Timestep 44 loss: 942693760.0\n",
      "Timestep 45 loss: 14486480896.0\n",
      "Timestep 46 loss: 214708715520.0\n",
      "Timestep 47 loss: 5802790223872.0\n",
      "Timestep 48 loss: 1191168835584.0\n",
      "Timestep 49 loss: 110969913344.0\n",
      "Timestep 50 loss: 174099005440.0\n",
      "Timestep 51 loss: 6551884005376.0\n",
      "Timestep 52 loss: 1804718309376.0\n",
      "Timestep 53 loss: 138693148672.0\n",
      "Timestep 54 loss: 3660803211264.0\n",
      "Timestep 55 loss: 697939787776.0\n",
      "Timestep 56 loss: 41626853376.0\n",
      "Timestep 57 loss: 1763982704640.0\n",
      "Timestep 58 loss: 20616744960.0\n",
      "Timestep 59 loss: 483817088.0\n",
      "Timestep 60 loss: 1919764736.0\n",
      "Timestep 61 loss: 10871760896.0\n",
      "Timestep 62 loss: 990645846016.0\n",
      "Timestep 63 loss: 101476409344.0\n",
      "Timestep 64 loss: 895944359936.0\n",
      "Timestep 65 loss: 45931580.0\n",
      "Timestep 66 loss: 287968788480.0\n",
      "Timestep 67 loss: 1794506227712.0\n",
      "Timestep 68 loss: 3646633279488.0\n",
      "Timestep 69 loss: 259170238464.0\n",
      "Timestep 70 loss: 3250904064.0\n",
      "Timestep 71 loss: 3393421049856.0\n",
      "Timestep 72 loss: 9639299072.0\n",
      "Timestep 73 loss: 24754368512.0\n",
      "Timestep 74 loss: 43295686656.0\n",
      "Timestep 75 loss: 3162394591232.0\n",
      "Timestep 76 loss: 147171606528.0\n",
      "Timestep 77 loss: 1964030558208.0\n",
      "Timestep 78 loss: 375307763712.0\n",
      "Timestep 79 loss: 274573184.0\n",
      "Timestep 80 loss: 235228971008.0\n",
      "Timestep 81 loss: 87788470272.0\n",
      "Timestep 82 loss: 4219342592.0\n",
      "Timestep 83 loss: 814023311360.0\n",
      "Timestep 84 loss: 8875676672.0\n",
      "Timestep 85 loss: 2006875897856.0\n",
      "Timestep 86 loss: 214031826944.0\n",
      "Timestep 87 loss: 563383369728.0\n",
      "Timestep 88 loss: 167053426688.0\n",
      "Timestep 89 loss: 1510674530304.0\n",
      "Timestep 90 loss: 1469836427264.0\n",
      "Timestep 91 loss: 1568436864.0\n",
      "Timestep 92 loss: 218836303872.0\n",
      "Timestep 93 loss: 101070088.0\n",
      "Timestep 94 loss: 50727325696.0\n",
      "Timestep 95 loss: 1792400818176.0\n",
      "Timestep 96 loss: 2483891456.0\n",
      "Timestep 97 loss: 928733462528.0\n",
      "Timestep 98 loss: 18671882240.0\n",
      "Timestep 99 loss: 30520418304.0\n",
      "Timestep 100 loss: 78685872.0\n",
      "Timestep 101 loss: 39939121152.0\n",
      "Timestep 102 loss: 7608869888.0\n",
      "Timestep 103 loss: 488508882944.0\n",
      "Timestep 104 loss: 148651245568.0\n",
      "Timestep 105 loss: 11598073856.0\n",
      "Timestep 106 loss: 3865375145984.0\n",
      "Timestep 107 loss: 266935943168.0\n",
      "Timestep 108 loss: 1814888185856.0\n",
      "Timestep 109 loss: 432682041344.0\n",
      "Timestep 110 loss: 107021238272.0\n",
      "Timestep 111 loss: 15563155456.0\n",
      "Timestep 112 loss: 197491310592.0\n",
      "Timestep 113 loss: 172619759616.0\n",
      "Timestep 114 loss: 803826237440.0\n",
      "Timestep 115 loss: 3527333117952.0\n",
      "Timestep 116 loss: 205321011200.0\n",
      "Timestep 117 loss: 25988849664.0\n",
      "Timestep 118 loss: 156015362048.0\n",
      "Timestep 119 loss: 1176600576000.0\n",
      "Timestep 120 loss: 66213208064.0\n",
      "Timestep 121 loss: 403904659456.0\n",
      "Timestep 122 loss: 240114892800.0\n",
      "Timestep 123 loss: 187712438272.0\n",
      "Timestep 124 loss: 889124618240.0\n",
      "Timestep 125 loss: 425820192768.0\n",
      "Timestep 126 loss: 455136444416.0\n",
      "Timestep 127 loss: 141052936192.0\n",
      "Timestep 128 loss: 277203025920.0\n",
      "Timestep 129 loss: 236954337280.0\n",
      "Timestep 130 loss: 18060130304.0\n",
      "Timestep 131 loss: 1666969108480.0\n",
      "Timestep 132 loss: 362019815424.0\n",
      "Timestep 133 loss: 31973322752.0\n",
      "Timestep 134 loss: 1852397846528.0\n",
      "Timestep 135 loss: 41487572992.0\n",
      "Timestep 136 loss: 104202526720.0\n",
      "Timestep 137 loss: 258879389696.0\n",
      "Timestep 138 loss: 780279873536.0\n",
      "Timestep 139 loss: 90020315136.0\n",
      "11027.249100167412 80.02334248679024 1545608.7741071428\n",
      "858990201480.9572 277215989501.6036\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_file = 'Cleaned_APS360_0012_dt0001_2.mat'\n",
    "test_data = scipy.io.loadmat(f\"./Data/{test_file}\")\n",
    "\n",
    "##### Bounds for testing ######\n",
    "start_time = 0\n",
    "end_time = 140\n",
    "\n",
    "# Change to linspace/add slicing here\n",
    "U_test = test_data['U_star'][...,start_time:end_time]  # 5000 x 2 x 200\n",
    "P_test = test_data['p_star'][...,start_time:end_time]  # 5000 x 200\n",
    "T_test = test_data['t'][start_time:end_time]  # 200 x 1\n",
    "X_test = test_data['X_star']  # 5000 x 2\n",
    "\n",
    "print(U_test.shape, P_test.shape, T_test.shape, X_test.shape)\n",
    "\n",
    "N = X_test.shape[0]\n",
    "T = T_test.shape[0]\n",
    "\n",
    "# Null vector to test against f and g, and error func\n",
    "null_vector = torch.zeros((N, 1))\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "u_loss_sum, v_loss_sum, p_loss_sum, f_loss_sum, g_loss_sum = [0]*5\n",
    "\n",
    "for time in range(T):\n",
    "    # Tidy up data\n",
    "    UU = U_test[:, 0, time]  # N x T\n",
    "    VV = U_test[:, 1, time]  # N x T\n",
    "    PP = P_test[...,time]  # N x T\n",
    "\n",
    "    TT = np.tile(T_test[time], (1, N))  # N x T\n",
    "\n",
    "    x_test = X_test[:, 0:1].flatten()[:, None]  # NT x 1\n",
    "    y_test = X_test[:, 1:2].flatten()[:, None]  # NT x 1\n",
    "    t_test = TT.flatten()[:, None]  # NT x 1\n",
    "\n",
    "    u_test = UU.flatten()[:, None]  # N x 1\n",
    "    v_test = VV.flatten()[:, None]  # N x 1\n",
    "    p_test = PP.flatten()[:, None]  # N x 1\n",
    "\n",
    "    # Convert test data to tensors\n",
    "    x = torch.tensor(x_test, dtype=torch.float32, requires_grad=True)\n",
    "    y = torch.tensor(y_test, dtype=torch.float32, requires_grad=True)\n",
    "    t = torch.tensor(t_test, dtype=torch.float32, requires_grad=True)\n",
    "    u = torch.tensor(u_test, dtype=torch.float32)\n",
    "    v = torch.tensor(v_test, dtype=torch.float32)\n",
    "    p = torch.tensor(p_test, dtype=torch.float32)\n",
    "    \n",
    "    # Get outputs\n",
    "    u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = create_pde(net, x, y, t)\n",
    "    u_loss = mse(u_prediction, u)\n",
    "    v_loss = mse(v_prediction, v)\n",
    "    p_loss = mse(p_prediction, p)\n",
    "    f_loss = mse(f_prediction, null_vector)\n",
    "    g_loss = mse(g_prediction, null_vector)\n",
    "\n",
    "    u_loss_sum += u_loss.item()\n",
    "    v_loss_sum += v_loss.item()\n",
    "    p_loss_sum += p_loss.item()\n",
    "    f_loss_sum += f_loss.item()\n",
    "    g_loss_sum += g_loss.item()\n",
    "\n",
    "    loss = u_loss + v_loss + p_loss + f_loss + g_loss\n",
    "\n",
    "    print(f\"Timestep {time} loss: {loss.item()}\")\n",
    "\n",
    "    # Reverse the manipulations done on input data\n",
    "    #u_prediction_rs = np.reshape(u_prediction.detach().numpy(), UU.shape)\n",
    "    #v_prediction_rs = np.reshape(v_prediction.detach().numpy(), VV.shape)\n",
    "    #p_prediction_rs = np.reshape(p_prediction.detach().numpy(), PP.shape)\n",
    "\n",
    "# Save to .mat for display\n",
    "#t_prediction_rs = T_test[start_time:end_time]\n",
    "#U_prediction_rs = np.stack([u_prediction_rs, v_prediction_rs], 1)\n",
    "\n",
    "#print(U_prediction_rs.shape, p_prediction_rs.shape)\n",
    "#scipy.io.savemat(f'./Results/run_{test_file}', {'U_star': U_prediction_rs, 'p_star': p_prediction_rs, 'X_star': X_test, 't': t_prediction_rs})\n",
    "\n",
    "print(u_loss_sum/T, v_loss_sum/T, p_loss_sum/T)\n",
    "print(f_loss_sum/T, g_loss_sum/T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14449, 2, 60) (14449, 60) (60, 1) (14449, 2)\n",
      "Timestep 0 loss: 2621784576.0\n",
      "Timestep 1 loss: 83512991744.0\n",
      "Timestep 2 loss: 50383314944.0\n",
      "Timestep 3 loss: 53539479552.0\n",
      "Timestep 4 loss: 1902124990464.0\n",
      "Timestep 5 loss: 2011882586112.0\n",
      "Timestep 6 loss: 100210745344.0\n",
      "Timestep 7 loss: 4886119841792.0\n",
      "Timestep 8 loss: 153933742080.0\n",
      "Timestep 9 loss: 2884036460544.0\n",
      "Timestep 10 loss: 1302356033536.0\n",
      "Timestep 11 loss: 1031577600.0\n",
      "Timestep 12 loss: 199836008448.0\n",
      "Timestep 13 loss: 62465900544.0\n",
      "Timestep 14 loss: 330449289216.0\n",
      "Timestep 15 loss: 291402350592.0\n",
      "Timestep 16 loss: 1116305024.0\n",
      "Timestep 17 loss: 25024942080.0\n",
      "Timestep 18 loss: 44019490816.0\n",
      "Timestep 19 loss: 1413400494080.0\n",
      "Timestep 20 loss: 194692972544.0\n",
      "Timestep 21 loss: 3653365661696.0\n",
      "Timestep 22 loss: 1756579758080.0\n",
      "Timestep 23 loss: 829999546368.0\n",
      "Timestep 24 loss: 6363115520.0\n",
      "Timestep 25 loss: 2123792384000.0\n",
      "Timestep 26 loss: 77554384896.0\n",
      "Timestep 27 loss: 497101307904.0\n",
      "Timestep 28 loss: 133022547968.0\n",
      "Timestep 29 loss: 1146654556160.0\n",
      "Timestep 30 loss: 233028157440.0\n",
      "Timestep 31 loss: 595679248384.0\n",
      "Timestep 32 loss: 995520348160.0\n",
      "Timestep 33 loss: 3949964288.0\n",
      "Timestep 34 loss: 4892016640.0\n",
      "Timestep 35 loss: 160172195840.0\n",
      "Timestep 36 loss: 85674196992.0\n",
      "Timestep 37 loss: 434118361088.0\n",
      "Timestep 38 loss: 3688827453440.0\n",
      "Timestep 39 loss: 910341570560.0\n",
      "Timestep 40 loss: 118476603392.0\n",
      "Timestep 41 loss: 16304228352.0\n",
      "Timestep 42 loss: 167923957760.0\n",
      "Timestep 43 loss: 190635180032.0\n",
      "Timestep 44 loss: 1812518928384.0\n",
      "Timestep 45 loss: 1712112664576.0\n",
      "Timestep 46 loss: 52036395008.0\n",
      "Timestep 47 loss: 1647293497344.0\n",
      "Timestep 48 loss: 3377845501952.0\n",
      "Timestep 49 loss: 169675358208.0\n",
      "Timestep 50 loss: 206724710400.0\n",
      "Timestep 51 loss: 5718167552.0\n",
      "Timestep 52 loss: 2099679330304.0\n",
      "Timestep 53 loss: 1532985475072.0\n",
      "Timestep 54 loss: 714273849344.0\n",
      "Timestep 55 loss: 79731671040.0\n",
      "Timestep 56 loss: 141492420608.0\n",
      "Timestep 57 loss: 6158915469312.0\n",
      "Timestep 58 loss: 282581860352.0\n",
      "Timestep 59 loss: 30790410240.0\n",
      "10929.968017578125 49.70005067189535 868459.5145833333\n",
      "414193687875.2 483280654549.3333\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_file = 'Cleaned_APS360_0012_dt0001_2.mat'\n",
    "test_data = scipy.io.loadmat(f\"./Data/{test_file}\")\n",
    "\n",
    "##### Bounds for testing ######\n",
    "start_time = 140\n",
    "end_time = 200\n",
    "\n",
    "# Change to linspace/add slicing here\n",
    "U_test = test_data['U_star'][...,start_time:end_time]  # 5000 x 2 x 200\n",
    "P_test = test_data['p_star'][...,start_time:end_time]  # 5000 x 200\n",
    "T_test = test_data['t'][start_time:end_time]  # 200 x 1\n",
    "X_test = test_data['X_star']  # 5000 x 2\n",
    "\n",
    "print(U_test.shape, P_test.shape, T_test.shape, X_test.shape)\n",
    "\n",
    "N = X_test.shape[0]\n",
    "T = T_test.shape[0]\n",
    "\n",
    "# Null vector to test against f and g, and error func\n",
    "null_vector = torch.zeros((N, 1))\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "u_loss_sum, v_loss_sum, p_loss_sum, f_loss_sum, g_loss_sum = [0]*5\n",
    "\n",
    "for time in range(T):\n",
    "    # Tidy up data\n",
    "    UU = U_test[:, 0, time]  # N x T\n",
    "    VV = U_test[:, 1, time]  # N x T\n",
    "    PP = P_test[...,time]  # N x T\n",
    "\n",
    "    TT = np.tile(T_test[time], (1, N))  # N x T\n",
    "\n",
    "    x_test = X_test[:, 0:1].flatten()[:, None]  # NT x 1\n",
    "    y_test = X_test[:, 1:2].flatten()[:, None]  # NT x 1\n",
    "    t_test = TT.flatten()[:, None]  # NT x 1\n",
    "\n",
    "    u_test = UU.flatten()[:, None]  # N x 1\n",
    "    v_test = VV.flatten()[:, None]  # N x 1\n",
    "    p_test = PP.flatten()[:, None]  # N x 1\n",
    "\n",
    "    # Convert test data to tensors\n",
    "    x = torch.tensor(x_test, dtype=torch.float32, requires_grad=True)\n",
    "    y = torch.tensor(y_test, dtype=torch.float32, requires_grad=True)\n",
    "    t = torch.tensor(t_test, dtype=torch.float32, requires_grad=True)\n",
    "    u = torch.tensor(u_test, dtype=torch.float32)\n",
    "    v = torch.tensor(v_test, dtype=torch.float32)\n",
    "    p = torch.tensor(p_test, dtype=torch.float32)\n",
    "    \n",
    "    # Get outputs\n",
    "    u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = create_pde(net, x, y, t)\n",
    "    u_loss = mse(u_prediction, u)\n",
    "    v_loss = mse(v_prediction, v)\n",
    "    p_loss = mse(p_prediction, p)\n",
    "    f_loss = mse(f_prediction, null_vector)\n",
    "    g_loss = mse(g_prediction, null_vector)\n",
    "\n",
    "    u_loss_sum += u_loss.item()\n",
    "    v_loss_sum += v_loss.item()\n",
    "    p_loss_sum += p_loss.item()\n",
    "    f_loss_sum += f_loss.item()\n",
    "    g_loss_sum += g_loss.item()\n",
    "\n",
    "    loss = u_loss + v_loss + p_loss + f_loss + g_loss\n",
    "\n",
    "    print(f\"Timestep {time} loss: {loss.item()}\")\n",
    "\n",
    "print(u_loss_sum/T, v_loss_sum/T, p_loss_sum/T)\n",
    "print(f_loss_sum/T, g_loss_sum/T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
