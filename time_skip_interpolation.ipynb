{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS Iteration 10, Loss: 0.2839852571487427\n",
      "LBFGS Iteration 20, Loss: 0.023229466751217842\n"
     ]
    }
   ],
   "source": [
    "''' This code follows the PINN - Navier Stokes project done by Computational Domain, but adapted for our secific scenarios'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "nu = 0.01\n",
    "batch_size = 1000  # Define a batch size to control memory usage\n",
    "\n",
    "# Load data and create training dataset\n",
    "data = scipy.io.loadmat('cylinder_wake.mat')\n",
    "\n",
    "U_star = data['U_star']  # N x 2 x T\n",
    "P_star = data['p_star']  # N x T\n",
    "t_star = data['t']  # T x 1\n",
    "X_star = data['X_star']  # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# Rearrange Data\n",
    "XX = np.tile(X_star[:, 0:1], (1, T))  # N x T\n",
    "YY = np.tile(X_star[:, 1:2], (1, T))  # N x T\n",
    "TT = np.tile(t_star, (1, N)).T  # N x T\n",
    "\n",
    "UU = U_star[:, 0, :]  # N x T\n",
    "VV = U_star[:, 1, :]  # N x T\n",
    "PP = P_star  # N x T\n",
    "\n",
    "x = XX.flatten()[:, None]  # NT x 1\n",
    "y = YY.flatten()[:, None]  # NT x 1\n",
    "t = torch.tensor(TT.flatten()[:, None], dtype=torch.float32, requires_grad=True)  # Convert to Tensor\n",
    "\n",
    "u = torch.tensor(UU.flatten()[:, None], dtype=torch.float32)  # Convert to Tensor\n",
    "v = torch.tensor(VV.flatten()[:, None], dtype=torch.float32)  # Convert to Tensor\n",
    "p = torch.tensor(PP.flatten()[:, None], dtype=torch.float32)  # Convert to Tensor\n",
    "\n",
    "# Skip every other time step\n",
    "skip_factor = 2\n",
    "t_skipped = t[::skip_factor]  # Skip every other time step\n",
    "u_skipped = u[::skip_factor]\n",
    "v_skipped = v[::skip_factor]\n",
    "p_skipped = p[::skip_factor]\n",
    "\n",
    "# Note: x and y remain the same, only the time-related data is skipped\n",
    "x_skipped = torch.tensor(np.tile(X_star[:, 0], T//skip_factor), dtype=torch.float32, requires_grad=True).reshape(-1, 1)\n",
    "y_skipped = torch.tensor(np.tile(X_star[:, 1], T//skip_factor), dtype=torch.float32, requires_grad=True).reshape(-1, 1)\n",
    "\n",
    "# Define a simple ResNet block\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x)) + x\n",
    "\n",
    "# Define the ResNet-style neural network with 7 hidden layers\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input_layer = nn.Linear(3, 20)\n",
    "        self.res_block1 = ResNetBlock(20, 20)\n",
    "        self.res_block2 = ResNetBlock(20, 20)\n",
    "        self.res_block3 = ResNetBlock(20, 20)\n",
    "        self.res_block4 = ResNetBlock(20, 20)\n",
    "        self.res_block5 = ResNetBlock(20, 20)\n",
    "        self.res_block6 = ResNetBlock(20, 20)\n",
    "        self.res_block7 = ResNetBlock(20, 20)\n",
    "        self.output_layer = nn.Linear(20, 3)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        x = self.res_block4(x)\n",
    "        x = self.res_block5(x)\n",
    "        x = self.res_block6(x)\n",
    "        x = self.res_block7(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "net = ResNet()\n",
    "\n",
    "# Define the loss function\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# function written by Computational Domain\n",
    "def create_pde(x, y, t):\n",
    "    res = net(torch.hstack((x, y, t)))\n",
    "    psi, p = res[:, 0:1], res[:, 1:2]\n",
    "\n",
    "    u = torch.autograd.grad(psi, y, grad_outputs=torch.ones_like(psi), create_graph=True)[0]  \n",
    "    v = -1. * torch.autograd.grad(psi, x, grad_outputs=torch.ones_like(psi), create_graph=True)[0]\n",
    "\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "\n",
    "    v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
    "    v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
    "\n",
    "    p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "\n",
    "    f = u_t + u * u_x + v * u_y + p_x - nu * (u_xx + u_yy)\n",
    "    g = v_t + u * v_x + v * v_y + p_y - nu * (v_xx + v_yy)\n",
    "\n",
    "    return u, v, p, f, g\n",
    "\n",
    "# Use LBFGS optimizer with early stopping\n",
    "optimizer_lbfgs = torch.optim.LBFGS(net.parameters(), lr=1, max_iter=20000, max_eval=50000,\n",
    "                                    history_size=50, tolerance_grad=1e-05, tolerance_change=0.5 * np.finfo(float).eps,\n",
    "                                    line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "loss_history = []\n",
    "f_residual_history = []\n",
    "g_residual_history = []\n",
    "lbfgs_iter = 0  # Initialize the LBFGS iteration counter\n",
    "patience = 200  # Number of iterations with no significant improvement before stopping\n",
    "min_delta = 1e-4  # Minimum change to qualify as an improvement\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Fine-tuning with LBFGS and early stopping in batches\n",
    "for i in range(0, x_skipped.shape[0], batch_size):\n",
    "    x_batch = x_skipped[i:i+batch_size].detach().clone().requires_grad_(True)\n",
    "    y_batch = y_skipped[i:i+batch_size].detach().clone().requires_grad_(True)\n",
    "    t_batch = t_skipped[i:i+batch_size].detach().clone().requires_grad_(True)\n",
    "    u_batch = u_skipped[i:i+batch_size]\n",
    "    v_batch = v_skipped[i:i+batch_size]\n",
    "    p_batch = p_skipped[i:i+batch_size]\n",
    "    \n",
    "    def closure():\n",
    "        global lbfgs_iter, best_loss, patience_counter  # Access the global iteration variables\n",
    "        optimizer_lbfgs.zero_grad()\n",
    "\n",
    "        u_prediction, v_prediction, p_prediction, f_prediction, g_prediction = create_pde(x_batch, y_batch, t_batch)\n",
    "        u_loss = mse(u_prediction, u_batch)\n",
    "        v_loss = mse(v_prediction, v_batch)\n",
    "        p_loss = mse(p_prediction, p_batch)\n",
    "        f_loss = mse(f_prediction, torch.zeros_like(f_prediction))\n",
    "        g_loss = mse(g_prediction, torch.zeros_like(g_prediction))\n",
    "\n",
    "        loss = u_loss + v_loss + p_loss + f_loss + g_loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Print the current iteration and loss\n",
    "        lbfgs_iter += 1\n",
    "        if lbfgs_iter % 10 == 0:\n",
    "            print(f'LBFGS Iteration {lbfgs_iter}, Loss: {loss.item()}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if loss.item() < best_loss - min_delta:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                return loss\n",
    "\n",
    "        # Track loss and residuals\n",
    "        loss_history.append(loss.item())\n",
    "        f_residual_history.append(f_loss.item())\n",
    "        g_residual_history.append(g_loss.item())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    optimizer_lbfgs.step(closure)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()\n",
    "\n",
    "# Plot f and g residuals\n",
    "plt.figure()\n",
    "plt.plot(f_residual_history, label='f Residual')\n",
    "plt.plot(g_residual_history, label='g Residual')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('f and g Residuals Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), 'pinn_skipped_timesteps_resnet_lbfgs.pt')\n",
    "\n",
    "# Load the trained model and evaluate\n",
    "net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "\n",
    "# Assuming necessary data and trained model are available:\n",
    "# x_train, y_train, t_train, u_train, v_train, p_train, net (the trained model)\n",
    "\n",
    "# Function to create and save an animation for a specific field on training data\n",
    "def create_training_animation(field_name, values_fn, vmin, vmax, filename):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Initialize the first frame using the first batch\n",
    "    batch_start = 0\n",
    "    batch_end = min(batch_size, x_skipped.shape[0])  # Set initial batch\n",
    "    x_batch = x_skipped[batch_start:batch_end]\n",
    "    y_batch = y_skipped[batch_start:batch_end]\n",
    "    t_batch = t_skipped[batch_start:batch_end]\n",
    "\n",
    "    t_frame = torch.tensor(np.full(x_batch.shape, t_batch[0].item()), dtype=torch.float32, requires_grad=True)\n",
    "    u_out, v_out, p_out, _, _ = create_pde(x_batch, y_batch, t_frame)\n",
    "    field_values = values_fn(u_out, v_out, p_out).detach().cpu().numpy().flatten()\n",
    "\n",
    "    contour = ax.tricontourf(x_batch.detach().numpy().flatten(), y_batch.detach().numpy().flatten(), field_values, levels=20, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    colorbar = fig.colorbar(contour, ax=ax, label=field_name)  # Add colorbar once\n",
    "\n",
    "    ax.set_xlim(x_batch.min().item(), x_batch.max().item())  # Set x-axis limits\n",
    "    ax.set_ylim(y_batch.min().item(), y_batch.max().item())  # Set y-axis limits\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'{field_name} at t = {t_batch[0][0].item()}')\n",
    "\n",
    "    def animate(i):\n",
    "        nonlocal contour, batch_start, batch_end, x_batch, y_batch, t_batch\n",
    "        for c in contour.collections:\n",
    "            c.remove()\n",
    "\n",
    "        # Update batch indices\n",
    "        batch_start = (i * batch_size) % x_skipped.shape[0]\n",
    "        batch_end = min(batch_start + batch_size, x_skipped.shape[0])\n",
    "\n",
    "        x_batch = x_skipped[batch_start:batch_end]\n",
    "        y_batch = y_skipped[batch_start:batch_end]\n",
    "        t_batch = t_skipped[batch_start:batch_end]\n",
    "\n",
    "        t_frame = torch.tensor(np.full(x_batch.shape, t_batch[0].item()), dtype=torch.float32, requires_grad=True)\n",
    "        u_out, v_out, p_out, _, _ = create_pde(x_batch, y_batch, t_frame)\n",
    "        field_values = values_fn(u_out, v_out, p_out).detach().cpu().numpy().flatten()\n",
    "\n",
    "        contour = ax.tricontourf(x_batch.detach().numpy().flatten(), y_batch.detach().numpy().flatten(), field_values, levels=20, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f'{field_name} at t = {t_batch[0][0].item()}')\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=min(100, t_skipped.shape[0] // batch_size), interval=200, blit=False)\n",
    "    ani.save(filename, writer='imagemagick', fps=10)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Determine value ranges for proper color scaling using the first batch\n",
    "t_frame = torch.tensor(np.full(x_skipped[:batch_size].shape, t_skipped[0].item()), dtype=torch.float32, requires_grad=True)\n",
    "u_out, v_out, p_out, _, _ = create_pde(x_skipped[:batch_size], y_skipped[:batch_size], t_frame)  # Extract only the first three values\n",
    "\n",
    "vmin_p, vmax_p = torch.min(p_out).item(), torch.max(p_out).item()\n",
    "vmin_u, vmax_u = torch.min(u_out).item(), torch.max(u_out).item()\n",
    "vmin_v, vmax_v = torch.min(v_out).item(), torch.max(v_out).item()\n",
    "\n",
    "# Create and save animations for each field on training data\n",
    "create_training_animation('Pressure Field', lambda u_out, v_out, p_out: p_out, vmin_p, vmax_p, 'pressure_training_animation.gif')\n",
    "create_training_animation('U Velocity Field', lambda u_out, v_out, p_out: u_out, vmin_u, vmax_u, 'u_velocity_training_animation.gif')\n",
    "create_training_animation('V Velocity Field', lambda u_out, v_out, p_out: v_out, vmin_v, vmax_v, 'v_velocity_training_animation.gif')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
